{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIxV6F0BEdhaLC58FVQFMD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgXQ62E7IruA"
      },
      "source": [
        "# **Tutorial**: PnP Example Using Manual Feature Matching\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![link text](https://docs.opencv.org/3.4/pnp.jpg)\n",
        "\n",
        "**Figure 1**: Points expressed in the world frame $X_w$ are projected into the image plane $[u,v]$ using the pinhole camera model (Figure from: https://docs.opencv.org/3.4/d5/d1f/calib3d_solvePnP.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "MaZjJnKb4Om8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classical feature detectors/descriptors\n",
        "A number of classical detectors are implemented in the OpenCV library. See this page for examples: https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html."
      ],
      "metadata": {
        "id": "7gKveCyKuqXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep-learning based detectors/descriptors\n",
        "A good survey on feature detectors that use deep-learning can be found here: https://arxiv.org/pdf/2401.17592"
      ],
      "metadata": {
        "id": "laOp9sAfu6dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mesh files\n",
        "\n",
        "\n",
        "We assume that the mesh file in `.ply` (or `.obj`) format are present in the directory `assets/`.\n",
        "\n",
        "The spaceship mesh file used in this tutorial was downloaded from: https://sketchfab.com/3d-models/spaceship-6164a883f57f4f13938c3c5999bc0e1f"
      ],
      "metadata": {
        "id": "YFY-xoxh4Ipm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkX7DiM6rmeM"
      },
      "source": [
        "## PyTorch3D in Colab\n",
        "\n",
        "Modules `torch` and `torchvision` are required. If `pytorch3d` is not installed, install it using the following cell. Here, I modified to install PyTorch3D from my own pre-built wheel. Using my own pytorch3d wheel allows for faster installation. Installing from source takes several minutes to complete.\n",
        "\n",
        "**‚ö†Ô∏è WARNING: If the PyTorch3D installation from the current wheel fails, create another one!!!**\n",
        "\n",
        "PyTorch3D takes a long time to install from source in Colab. Instead of installing from source everytime an Colab instance is started, this notebook uses a pre-built whell. The pre-built PyTorch3D wheel is downloaded from my Dropbox (shared link). Another copy of the wheel is also stored in my Google Drive, and is located at: `/content/drive/MyDrive/research/projects/slosh_project/slosh_project_team_files/Colab_wheels/`\n",
        "\n",
        "## Load the CAD model file\n",
        "\n",
        "We will load a CAD model (e.g., `ply` or `obj`) file and create a **Meshes** object. **Meshes** is a unique datastructure provided in PyTorch3D for working with **batches of meshes of different sizes**. It has several useful class methods which are used in the rendering pipeline.\n",
        "\n",
        "## Create a renderer\n",
        "\n",
        "A **renderer** in PyTorch3D is composed of a **rasterizer** and a **shader** which each have a number of subcomponents such as a **camera** (orthographic/perspective). Here, we initialize some of these components and use default values for the rest.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General settings (User input)\n",
        "\n",
        "\n",
        "‚ö†Ô∏è <b>Attention:</b> Replace the information with your GitHub email and username.\n",
        "\n",
        "\n",
        "‚ö†Ô∏è <b>Attention:</b> Press enter or run cells to accept default values.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "--AKB1LOLlpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings for GitHub Access\n",
        "\n",
        "# Set name and email for github cloning using #@param\n",
        "git_username = \"eraldoribeiro\" #@param {type:\"string\"}\n",
        "git_email = \"eribeiro@fit.edu\" #@param {type:\"string\"}\n",
        "\n",
        "repository_name = \"point3D_from_depth\" #@param {type:\"string\"}\n",
        "organization_name = \"ribeiro-computer-vision\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jae248wvLkJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to PyTorch3D (pre-built) wheel\n",
        "\n",
        "# Set name and email for github cloning using #@param\n",
        "dropbox_link = \"https://www.dropbox.com/scl/fi/fqvlnyponcbekjd01omhj/pytorch3d-0.7.8-cp312-cp312-linux_x86_64.whl?rlkey=563mfx35rog42z1c8y7qn31sk&dl=0\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "JPPOrO2KaAWB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to the Mast3r checkpoints file (Google Drive)\n",
        "\n",
        "checkpoints_gdrive_path = \"/content/drive/MyDrive/teaching/tutorials_files/mast3r_checkpoints\" #@param {type:\"string\"}\n",
        "\n",
        "checkpoints_file_name =  \"MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SualFq2Fqhg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to mesh file (.obj)\n",
        "\n",
        "# Path to mesh file\n",
        "obj_path = \"point3D_from_depth/assets/StarShip_small.obj\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u8YYtzbygADZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì∑ Camera Intrinsics\n",
        "# Focal lengths\n",
        "focal_length_x = 900  #@param {type:\"number\"}\n",
        "focal_length_y = 900  #@param {type:\"number\"}\n",
        "\n",
        "# Principal point\n",
        "principal_point_x = 128  #@param {type:\"number\"}\n",
        "principal_point_y = 128  #@param {type:\"number\"}\n",
        "\n",
        "# Image dimensions\n",
        "image_witdh = 256   #@param {type:\"number\"}\n",
        "image_height = 256  #@param {type:\"number\"}\n",
        "\n",
        "# --- Aliases for convenience in code ---\n",
        "fx, fy = focal_length_x, focal_length_y\n",
        "cx, cy = principal_point_x, principal_point_y\n",
        "\n",
        "print(\"\\nK =\")\n",
        "print(f\"[[{fx:8.2f} {0.0:8.2f} {cx:8.2f}]\")\n",
        "print(f\" [{0.0:8.2f} {fy:8.2f} {cy:8.2f}]\")\n",
        "print(f\" [{0.0:8.2f} {0.0:8.2f} {1.0:8.2f}]]\\n\")\n",
        "\n",
        "# --- Aliases for convenience in code ---\n",
        "W = image_witdh\n",
        "H = image_height"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ybeL8nQ7g1zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1. ‚öôÔ∏è Setting up\n"
      ],
      "metadata": {
        "id": "jT1eWhCKCEzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to True if you want to mount gdrive\n",
        "mount_gdrive = True"
      ],
      "metadata": {
        "id": "BrwWhPGGGu6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWsl7eP3KdNj"
      },
      "outputs": [],
      "source": [
        "!pip --quiet install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÇ Clone Repository & üîë Mount Google Drive  & Install PyTorch3D/dependencies\n",
        "\n",
        "Clone the repository and mount **Google Drive** (requires user interaction).  \n",
        "This will also set up the environment and install the necessary libraries.\n"
      ],
      "metadata": {
        "id": "FSjobg6zKdNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set name and email for github cloning**\n",
        "\n",
        "<div style=\"border-left: 5px solid #FFA500; padding: 12px; background-color: #FFF4E5; font-size: 18px;\">\n",
        "  ‚ö†Ô∏è <b>Attention:</b> Replace the information with your GitHub email and username.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "KJ8iVQS3GItA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name git_username\n",
        "!git config --global user.email git_email"
      ],
      "metadata": {
        "id": "W1cT5w9BKdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîΩ Clone the Repository\n",
        "\n",
        "The next cell will **clone the repository** containing the notebooks and helper functions you‚Äôll need.  \n",
        "\n",
        "If the command fails (for example, due to missing secrets or permissions), you can open a **Terminal** in Colab and manually run the `git clone` command there.\n"
      ],
      "metadata": {
        "id": "46bBm7xFKdNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gh_clone(user, repo, token_key=\"GH_TOKEN\"):\n",
        "    from google.colab import userdata\n",
        "    token = userdata.get(token_key)\n",
        "    url = f\"https://{user}:{token}@github.com/{user}/{repo}.git\"\n",
        "    !git clone $url\n",
        "    %cd $repo\n",
        "    !git remote set-url origin $url\n",
        "    del token\n"
      ],
      "metadata": {
        "id": "spkzwj5DKdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell will **clone the repository** containing this notebooks and helper functions you‚Äôll need.\n",
        "\n",
        "If the `git clone` command fails (for example, due to missing secrets or permissions), you can open a **Terminal** in Colab and manually run the `git clone` command there.\n",
        "\n",
        "In Colab, we can only open a current notebook. But, we can edit python files (containing our library of functions) using git as we would normally when working on a computer. Any changes to files will only be saved to GitHub if we commit/push the changes prior to disconnecting the Colab instance. Colab sometimes disconnects without a warning so make sure the changes to files or notebooks are saved to github or google drive.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDLfvLkGV_M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gh_clone(organization_name, repository_name)\n",
        "\n",
        "# ‚úÖ Verify that the repository was cloned\n",
        "import os\n",
        "repo_name = \"/content/\" + repository_name\n",
        "if os.path.exists(repo_name):\n",
        "    print(f\"‚úÖ Repository '{repo_name}' successfully cloned!\")\n",
        "else:\n",
        "    print(f\"‚ùå Repository '{repo_name}' not found. Try cloning manually.\")"
      ],
      "metadata": {
        "id": "-50qkaXdV_NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîΩ Mount google drive"
      ],
      "metadata": {
        "id": "ZWoxDdyDKdNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaEFvu-dKdNk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "\n",
        "# auth.authenticate_user()\n",
        "\n",
        "local_path = os.getcwd()\n",
        "print(\"Current local path:\", local_path)\n",
        "\n",
        "# Mount google drive if using Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    local_path = \"/content/\"\n",
        "    from google.colab import drive\n",
        "    if mount_gdrive:\n",
        "        if mount_gdrive:\n",
        "            drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "\n",
        "os.chdir(local_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Install Pytorch3D"
      ],
      "metadata": {
        "id": "x6UF4bedKdNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ö° Install PyTorch3D from Wheel\n",
        "\n",
        "PyTorch3D installation can take longer than 8-10 minutes when installed from source.\n",
        "\n",
        "Here, **PyTorch3D is installed from a wheel** for a faster setup of about 2 minutes in Colab.\n",
        "\n",
        "- If the installer instead tries to **build from source**, it means the wheel is outdated or missing.  \n",
        "- In that case, you can **create your own wheel directly in Colab**, save it to **Google Drive** (or Dropbox), and reuse it later for faster installation.\n",
        "- To create your own PyTorch3D wheel in Colab, follow the instructions in the cell after these installation cells.\n",
        "\n"
      ],
      "metadata": {
        "id": "gmNGyDXgOqWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  ---------------------------- IMPORTS -----------------------------------------\n",
        "# Stdlib\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Literal, Dict, Any\n",
        "\n",
        "# Third-party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import imageio\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from tqdm.notebook import tqdm\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "# set path for libraries\n",
        "sys.path.append(repo_name)\n"
      ],
      "metadata": {
        "id": "gBN4D_j3KdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbDXl1qaKdNk"
      },
      "outputs": [],
      "source": [
        "# --- Config ---\n",
        "mount_gdrive = False\n",
        "\n",
        "# --- Imports ---\n",
        "import importlib, os, sys, shutil, subprocess, urllib.request, pathlib\n",
        "import installation_tools as install_tools\n",
        "importlib.reload(install_tools)\n",
        "\n",
        "# --- Short helpers (no notebook magics) ---\n",
        "def run(cmd, check=True):\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    try:\n",
        "        subprocess.run(cmd, check=check)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed ({e.returncode}): {' '.join(cmd)}\")\n",
        "        if check:\n",
        "            raise\n",
        "\n",
        "def pip_install(*pkgs, extra=None, check=True):\n",
        "    args = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
        "    if extra:\n",
        "        args += extra\n",
        "    args += list(pkgs)\n",
        "    run(args, check=check)\n",
        "\n",
        "def conda_available():\n",
        "    return shutil.which(\"conda\") is not None\n",
        "\n",
        "def conda_install(*pkgs):\n",
        "    if not conda_available():\n",
        "        print(\"conda not available; skipping conda installs.\")\n",
        "        return\n",
        "    # Use -c conda-forge channel and auto-yes\n",
        "    run([\"conda\", \"install\", \"-y\", \"-c\", \"conda-forge\", *pkgs], check=False)\n",
        "\n",
        "# --- Detect platform ---\n",
        "pm = install_tools.PlatformManager()\n",
        "platform, local_path = pm.platform, pm.local_path\n",
        "print(\"Detected:\", platform, local_path)\n",
        "\n",
        "# --- Optional: Mount GDrive if on Colab ---\n",
        "if mount_gdrive and platform == \"Colab\":\n",
        "    pm.mount_gdrive()\n",
        "\n",
        "# --- Lightning AI specific environment tweaks ---\n",
        "if platform == \"LightningAI\":\n",
        "    # conda piece (if conda exists in the image)\n",
        "    conda_install(\"libstdcxx-ng=13\")\n",
        "    # pip pins / extras\n",
        "    pip_install(\"numpy<2.0\", check=False)\n",
        "    pip_install(\"scikit-image\", \"gradio\", \"moviepy\", \"plotly\", check=False)\n",
        "    # If requirements.txt exists in CWD, install it\n",
        "    if os.path.exists(\"requirements.txt\"):\n",
        "        pip_install(\"-r\", \"requirements.txt\")\n",
        "\n",
        "# --- Install PyTorch3D (handles platform differences & fallbacks) ---\n",
        "installer = install_tools.PyTorch3DInstaller(\n",
        "    platform, local_path, dropbox_wheel_url=dropbox_link\n",
        ")\n",
        "installer.install()\n",
        "\n",
        "\n",
        "\n",
        "# --- Extra libraries (quiet-ish) ---\n",
        "# Original line had: trimesh pyrender opencv-python matplotlib pytorch-lightning\n",
        "pip_install(\"trimesh\", \"pyrender\", \"opencv-python\", \"matplotlib\", \"pytorch-lightning\", check=False)\n",
        "\n",
        "# --- Download plot_image_grid.py if missing ---\n",
        "filename = \"plot_image_grid.py\"\n",
        "url = \"https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\"\n",
        "if not os.path.exists(filename):\n",
        "    print(f\"Downloading {filename} ...\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(\"Saved to\", pathlib.Path(filename).resolve())\n",
        "    except Exception as e:\n",
        "        print(\"Download failed:\", e)\n",
        "\n",
        "# --- gdown ---\n",
        "pip_install(\"gdown\", extra=[\"--quiet\"], check=False)\n",
        "print(\"‚úÖ Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install and import colorama module (color printing)**"
      ],
      "metadata": {
        "id": "0U1UTXH6REb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "from colorama import Fore, Back, Style, init\n",
        "\n",
        "# ---------- pretty print helpers ----------\n",
        "RESET=\"\\033[0m\"; BOLD=\"\\033[1m\"\n",
        "C={\"ok\":\"\\033[1;32m\",\"info\":\"\\033[1;36m\",\"step\":\"\\033[1;35m\",\"warn\":\"\\033[1;33m\"}\n",
        "CYAN  = \"\\033[1;36m\"; GREEN = \"\\033[1;32m\"; YELLOW = \"\\033[1;33m\"\n",
        "\n",
        "\n",
        "def say(kind,msg): print(f\"{C[kind]}{msg}{RESET}\")\n",
        "torch.set_printoptions(precision=4, sci_mode=False)\n",
        "np.set_printoptions(precision=4, suppress=True)\n"
      ],
      "metadata": {
        "id": "aAPNSXLArV1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### üõ†Ô∏è (Optional) Build Your Own PyTorch3D Wheel\n",
        "\n",
        "If the pre-built wheel does not match your setup, you can **build PyTorch3D from source** and save the wheel to Google Drive.  \n",
        "This way, you only build once and reuse the `.whl` file in future Colab sessions.\n",
        "\n"
      ],
      "metadata": {
        "id": "VmfoO-IT1GWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Steps\n",
        "\n",
        "#### üîΩ 1. Clone PyTorch3D\n",
        "```python\n",
        "!git clone https://github.com/facebookresearch/pytorch3d.git\n",
        "%cd pytorch3d\n",
        "```\n",
        "#### üîΩ 2. Build the wheel (this may take several minutes)\n",
        "``` python\n",
        "!pip install build\n",
        "!python -m build --wheel\n",
        "```\n",
        "\n",
        "#### üîΩ 3. Find the wheel file\n",
        "``` python\n",
        "import glob, os\n",
        "wheels = glob.glob(\"dist/*.whl\")\n",
        "print(\"üì¶ Built wheels:\", wheels)\n",
        "```\n",
        "\n",
        "#### üîΩ 4. Copy the wheel to Google Drive (adjust path if needed)\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_path = \"/content/drive/MyDrive/pytorch3d_wheels/\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "for w in wheels:\n",
        "    !cp $w $save_path\n",
        "print(\"‚úÖ Wheel(s) saved to:\", save_path)\n",
        "```"
      ],
      "metadata": {
        "id": "U88mr5ZG3JN9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzgLJbG0BMak"
      },
      "source": [
        "#### PyTorch3D imports\n",
        "The following cell require PyTorch3D. Ensure it is executed after PyTorch3D is installed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ---------------------------- IMPORTS -----------------------------------------\n",
        "# PyTorch3D ‚Äî IO & data structures\n",
        "from pytorch3d.io import load_obj, load_ply, load_objs_as_meshes\n",
        "from pytorch3d.structures import Meshes\n",
        "\n",
        "# PyTorch3D ‚Äî transforms\n",
        "from pytorch3d.transforms import Rotate, Translate\n",
        "\n",
        "# PyTorch3D ‚Äî rendering\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    PerspectiveCameras,\n",
        "    look_at_view_transform,\n",
        "    look_at_rotation,\n",
        "    camera_position_from_spherical_angles,\n",
        "    RasterizationSettings,\n",
        "    MeshRenderer,\n",
        "    MeshRasterizer,\n",
        "    BlendParams,\n",
        "    SoftSilhouetteShader,\n",
        "    SoftPhongShader,\n",
        "    HardPhongShader,\n",
        "    PointLights,\n",
        "    DirectionalLights,\n",
        "    Materials,\n",
        "    TexturesUV,\n",
        "    TexturesVertex,\n",
        ")\n",
        "from pytorch3d.renderer.cameras import CamerasBase\n",
        "\n",
        "# PyTorch3D ‚Äî visualization helpers (optional)\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
        "\n",
        "# Project utils path (adjust as needed)\n",
        "sys.path.append(os.path.abspath(''))\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "qTLParFcKdNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions\n",
        "These function require PyTorch3D. As a result, they must be declared after PyTorch3D is installed."
      ],
      "metadata": {
        "id": "Uvg2B5WNKdNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import my own libraries and helper functions**\n"
      ],
      "metadata": {
        "id": "tlVxqQc739tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unproject_3d_from_depth_tools as unproject_tools\n",
        "importlib.reload(unproject_tools)\n",
        "\n",
        "import my_mast3r_setup as my_mast3r_tools\n",
        "importlib.reload(my_mast3r_tools)\n",
        "\n",
        "import tools_pytorch3d_coordsystems_demo as myp3dtools\n",
        "importlib.reload(unproject_tools)"
      ],
      "metadata": {
        "id": "zP_dmR7P36kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "SLIn2I6LLxp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def mast3r_match_features(template_image, cams_template, depth_template, target_image, model):\n",
        "\n",
        "\n",
        "    # We need these for later in the program\n",
        "    device = cams_template.R.device\n",
        "    dtype  = cams_template.R.dtype\n",
        "    # imgsz  = torch.tensor([[H, W]], device=device)\n",
        "\n",
        "    # Save rgb image to file for now to test feature detection directly.\n",
        "    # TODO: Change load_images function or create a new version that accepts image arrays instead of file names/directories\n",
        "    # img is a numpy array (H,W,3)\n",
        "    plt.imsave(path_to_images + \"template_image.png\", template_image)\n",
        "\n",
        "    # Save rgb image to file for now to test feature detection directly.\n",
        "    # Change load_images function or create a new version that accepts image arrays instead of file names/directories\n",
        "    # img is a numpy array (H,W,3)\n",
        "    plt.imsave(path_to_images + \"target_image.png\", target_image)\n",
        "\n",
        "    # Load image into list to create input for feature matcher\n",
        "    images = load_images([path_to_images + 'template_image.png', path_to_images + 'target_image.png'], size=256, square_ok=True)\n",
        "    print(images[0][\"img\"].shape)\n",
        "\n",
        "    print(\"***-------------------------------------------------------------- ***\")\n",
        "    print(\"**  3. Feature matching                                            **\")\n",
        "    print(\"***-------------------------------------------------------------- ***\")\n",
        "\n",
        "    # Inference\n",
        "    matches_template, matches_target, \\\n",
        "    view_template, pred_template, \\\n",
        "    view_target, pred_target = featmatchtools.FeatureMatcher.mast3r_inference(images = images,\n",
        "                                                              model = model,\n",
        "                                                              device = device)\n",
        "\n",
        "    # convert the view output from mast3r to standard rgb image\n",
        "    v_template = featmatchtools.FeatureMatcher.mast3r_view2rgbimage(view_template)\n",
        "    v_target   = featmatchtools.FeatureMatcher.mast3r_view2rgbimage(view_target)\n",
        "\n",
        "    # Create binary mask for this view (used for filtering spurious matches)\n",
        "    v_template_mask = unproject_tools.ImageProcessor.make_binary_mask(v_template, white_tol= 0, black_tol = 0)  # returns 0/1 mask\n",
        "    v_template_mask = featmatchtools.FeatureMatcher.smooth_and_fill_mask(v_template_mask)\n",
        "\n",
        "    # Create a binary mask for this view (used for filtering spurious matches)\n",
        "    v_target_mask = unproject_tools.ImageProcessor.make_binary_mask(v_target, white_tol= 0, black_tol = 0)  # returns 0/1 mask\n",
        "    v_target_mask = featmatchtools.FeatureMatcher.smooth_and_fill_mask(v_target_mask)\n",
        "\n",
        "    # Obtain a clean set of matches for each image (no matches in background)\n",
        "    matches_template_fg, matches_target_fg = featmatchtools.FeatureMatcher.filter_matches_by_mask(matches_template, matches_target, v_template_mask, v_target_mask)\n",
        "\n",
        "    # These (u,v) points are on the template image (coarse pose).\n",
        "    # They are the input to the 3-D unprojector from depth\n",
        "    points_uv_template = matches_template_fg\n",
        "\n",
        "    # Calculate depths for each (u,v) template point. We do not use this function\n",
        "    # to calculate 3-D coordinates or camera coordinates. However, we keep X_world\n",
        "    # for now, as we need it to pass to the clean-up function in the next step.\n",
        "    # We ignore camera coordinates as we use PyTorch3D's unproject_points() for\n",
        "    # the actual recovery of 3-D points from depth.\n",
        "    fx = cams_template.focal_length.cpu().numpy().squeeze()[0]\n",
        "    fy = cams_template.focal_length.cpu().numpy().squeeze()[1]\n",
        "    cx = cams_template.principal_point.cpu().numpy().squeeze()[0]\n",
        "    cy = cams_template.principal_point.cpu().numpy().squeeze()[1]\n",
        "    _, X_world, depths = \\\n",
        "      unproject_tools.Unprojector.recover_3D_points(matches_template_fg,\n",
        "                                                    depth_template.cpu(),\n",
        "                                                    fx, fy, cx, cy,\n",
        "                                                    cams_template.R,\n",
        "                                                    cams_template.T)\n",
        "\n",
        "    # Remove the matches for which 3-D reprojected coordinates have nan\n",
        "    # uv_clean are corresponding points to the actual test image (i.e., cutout image)\n",
        "    _, _, kept_idx, removed_idx = featmatchtools.FeatureMatcher.filter_nan_points(X_world, matches_template_fg)\n",
        "\n",
        "\n",
        "    template_points = matches_template_fg[kept_idx]\n",
        "    target_points = matches_target_fg[kept_idx]\n",
        "\n",
        "    return template_points, target_points, view_template, view_target\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Demo function to use for tests\n",
        "def create_and_display_image(distance=3,\n",
        "                             elev=0,\n",
        "                             azim=0,\n",
        "                             roll=0,\n",
        "                             K=np.eye(3),\n",
        "                             H=256,\n",
        "                             W=256):\n",
        "\n",
        "    # Get intrinsics\n",
        "    fx = K[0,0]\n",
        "    fy = K[1,1]\n",
        "    cx = K[0,2]\n",
        "    cy = K[1,2]\n",
        "\n",
        "    # Create rgb and depth images\n",
        "    rgb, depth, cams = unproject_tools.RenderWithPytorch3D.render_rgb_depth_from_view(\n",
        "        mesh,\n",
        "        fx=fx, fy=fy, cx=cx, cy=cy,\n",
        "        width=W, height=H,\n",
        "        distance=distance, elev=elev, azim=azim, roll_deg=roll,\n",
        "        roll_mode=\"camera\",   # try \"camera\" if you prefer or \"world\".\n",
        "    )\n",
        "\n",
        "    # Show\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1); plt.imshow(np.clip(rgb,0,1)); plt.axis('off'); plt.title('RGB')\n",
        "\n",
        "    # Depth visualization (treat -1 as invalid)\n",
        "    vis = unproject_tools.ImageProcessor.depth_to_rgb(depth, cmap=\"plasma\", bg_mode=\"white\")\n",
        "\n",
        "    plt.subplot(1,2,2); plt.imshow(vis); plt.axis('off'); plt.title('Depth')\n",
        "    plt.show()\n",
        "\n",
        "    #------------------------------- Show results ---------------------------------\n",
        "    myp3dtools.overlay_axes_p3d(rgb, cams, 256, 256,\n",
        "                    world_origin=(0,0,0), axis_len=0.3,\n",
        "                    draw_world_axes=True, draw_camera_axes=False,\n",
        "                    cam_axis_len=0.5,\n",
        "                    title=\"PyTorch3D camera\")\n",
        "\n",
        "    # Pretty print camera information\n",
        "    myp3dtools.print_camera_pose_matrices(cams.R, cams.T, \"*** PyTorch3D Camera ***\")\n",
        "\n",
        "    # clear gpu cache\n",
        "    unproject_tools.Util.clear_cuda_cache()\n",
        "\n",
        "    return rgb, depth, cams\n",
        "\n"
      ],
      "metadata": {
        "id": "PtSBhRlhsUOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CIbhy_uKdNl"
      },
      "source": [
        "### **Setup feature matcher (MAST3r)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install MAST3R\n",
        "my_mast3r_tools.setup_mast3r(checkpoint_gdrive_dir=checkpoints_gdrive_path,\n",
        "                             ckpt_name=checkpoints_file_name)\n",
        "\n",
        "# Create mast3r_images directory containing images for mast3r\n",
        "mast3r_images_dir = local_path + \"mast3r_images/\"\n",
        "if not os.path.exists(mast3r_images_dir):\n",
        "    os.makedirs(mast3r_images_dir)\n",
        "\n",
        "\n",
        "## Imports (from inside the mast3r directory)\n",
        "from mast3r.model import AsymmetricMASt3R\n",
        "from mast3r.fast_nn import fast_reciprocal_NNs\n",
        "\n",
        "import mast3r.utils.path_to_dust3r\n",
        "from dust3r.inference import inference\n",
        "from dust3r.utils.image import load_images\n",
        "\n",
        "\n",
        "# visualize a few matches\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms.functional\n",
        "from matplotlib import pyplot as pl\n",
        "\n",
        "import cv2\n",
        "import feature_matcher_tools as featmatchtools\n",
        "importlib.reload(featmatchtools)"
      ],
      "metadata": {
        "id": "jBqfv85eKdNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    path_to_images = \"/content/mast3r_images/\"\n",
        "else:\n",
        "    path_to_images = \"/teamspace/studios/this_studio/mast3r_images/\"\n",
        "\n",
        "print(f\"Path to image data is: {path_to_images}\")"
      ],
      "metadata": {
        "id": "szbKJMJzCetY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "schedule = 'cosine'\n",
        "lr = 0.01\n",
        "niter = 300\n",
        "\n",
        "model_name = \"naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric\"\n",
        "# you can put the path to a local checkpoint in model_name if needed\n",
        "model = AsymmetricMASt3R.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "6hiH6jnJGqZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read cad file into a PyTorch3D mesh"
      ],
      "metadata": {
        "id": "G_7s0Zc8KdNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "### Read cad file\n",
        "\n",
        "# Ensure we are in the home path\n",
        "os.chdir(local_path)\n",
        "\n",
        "# Load mesh\n",
        "mesh = load_objs_as_meshes([obj_path], device=device)"
      ],
      "metadata": {
        "id": "qYDBADlcKdNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the camera\n",
        "Ensure that this the only camera. PnP camera and rendering cameras must be the same."
      ],
      "metadata": {
        "id": "P8_nRIagKdNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Different type of matrices for different functions\n",
        "\n",
        "# Numpy K\n",
        "K_np = np.array([[fx, 0, cx],\n",
        "                 [0, fy, cy],\n",
        "                 [0,  0,  1]], dtype=np.float32)\n",
        "\n",
        "# Torch K\n",
        "K_torch = torch.tensor(K_np, dtype=torch.float32, device=device).unsqueeze(0)  # (1,3,3)"
      ],
      "metadata": {
        "id": "As2oQ2qvLnkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "kr7AyYHDTTQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def prepare_uvd_for_unproject(template_points, depth_template):\n",
        "\n",
        "    # Get the interpolated depths for the list of (u,v) points\n",
        "    points_depth = [unproject_tools.Unprojector.bilinear_sample_depth(depth_template.cpu(), uv) for uv in template_points]\n",
        "\n",
        "    # Create the (u,v,d) to pass to unproject where d = depth\n",
        "    z_cam = np.array([\n",
        "      points_depth\n",
        "    ], dtype=np.float32).T\n",
        "\n",
        "    # Convert from list to np.array\n",
        "    uv = np.array([\n",
        "      template_points\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    # Concatenate (u,v) and depth to form (u,v,depth)\n",
        "    uvd1 = np.concatenate([uv.squeeze(), z_cam], axis=1)\n",
        "\n",
        "    # Flip image axes using image size (2-D flip, not 3-D)\n",
        "    uvd1[:,0] = -uvd1[:,0] + W\n",
        "    uvd1[:,1] = -uvd1[:,1] + H\n",
        "\n",
        "\n",
        "    return uvd1\n",
        "\n",
        "\n",
        "def recover_3D_from_depth_map(template_points, depth_template, cams_template):\n",
        "\n",
        "\n",
        "    # # Create the (u,v,d) to pass to unproject where d = depth.\n",
        "    # # Actually, we need to pass (W-u, H-v, cam_depth)\n",
        "    uvd = prepare_uvd_for_unproject(template_points, depth_template)\n",
        "\n",
        "    # Use Pytorch3D to unproject (u,v,depth) to camera coordinates\n",
        "    # (i.e., world_coordinates = False) for our camera.\n",
        "    Xcam_unproject_t = cams_template.unproject_points(torch.tensor(uvd, device=\"cuda:0\", dtype=dtype),\n",
        "                                              world_coordinates=False)\n",
        "\n",
        "    # Also, unproject (u,v,depth) to world coordinates\n",
        "    # (i.e., world_coordinates = True) for our camera.\n",
        "    Xworld_unproject_t = cams_template.unproject_points(torch.tensor(uvd, device=\"cuda:0\", dtype=dtype),\n",
        "                                              world_coordinates=True)\n",
        "\n",
        "    return Xworld_unproject_t\n",
        "\n",
        "# Demo function to use for tests\n",
        "def create_and_display_image(distance=3,\n",
        "                             elev=0,\n",
        "                             azim=0,\n",
        "                             roll=0,\n",
        "                             K=np.eye(3),\n",
        "                             H=256,\n",
        "                             W=256):\n",
        "\n",
        "    # Get intrinsics\n",
        "    fx = K[0,0]\n",
        "    fy = K[1,1]\n",
        "    cx = K[0,2]\n",
        "    cy = K[1,2]\n",
        "\n",
        "    # Create rgb and depth images\n",
        "    rgb, depth, cams = unproject_tools.RenderWithPytorch3D.render_rgb_depth_from_view(\n",
        "        mesh,\n",
        "        fx=fx, fy=fy, cx=cx, cy=cy,\n",
        "        width=W, height=H,\n",
        "        distance=distance, elev=elev, azim=azim, roll_deg=roll,\n",
        "        roll_mode=\"camera\",   # try \"camera\" if you prefer or \"world\".\n",
        "    )\n",
        "\n",
        "    # Show\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1); plt.imshow(np.clip(rgb,0,1)); plt.axis('off'); plt.title('RGB')\n",
        "\n",
        "    # Depth visualization (treat -1 as invalid)\n",
        "    vis = unproject_tools.ImageProcessor.depth_to_rgb(depth, cmap=\"plasma\", bg_mode=\"white\")\n",
        "\n",
        "    plt.subplot(1,2,2); plt.imshow(vis); plt.axis('off'); plt.title('Depth')\n",
        "    plt.show()\n",
        "\n",
        "    #------------------------------- Show results ---------------------------------\n",
        "    myp3dtools.overlay_axes_p3d(rgb, cams, 256, 256,\n",
        "                    world_origin=(0,0,0), axis_len=0.3,\n",
        "                    draw_world_axes=True, draw_camera_axes=False,\n",
        "                    cam_axis_len=0.5,\n",
        "                    title=\"PyTorch3D camera\")\n",
        "\n",
        "    # Pretty print camera information\n",
        "    myp3dtools.print_camera_pose_matrices(cams.R, cams.T, \"*** PyTorch3D Camera ***\")\n",
        "\n",
        "    # clear gpu cache\n",
        "    unproject_tools.Util.clear_cuda_cache()\n",
        "\n",
        "    return rgb, depth, cams\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def add_subpixel_noise(arr, H, W, scale=0.5, scale_x=None, scale_y=None, seed=None):\n",
        "    \"\"\"\n",
        "    Add small Gaussian subpixel noise to the first two columns (x,y) of arr.\n",
        "\n",
        "    Args:\n",
        "        arr    : (N,3) numpy array, where first 2 cols are image coords\n",
        "        H, W   : image height, width (used to clip coords inside image)\n",
        "        scale  : std dev of noise in pixels (applied if scale_x/scale_y not set)\n",
        "        scale_x: std dev for x-axis noise (overrides scale if provided)\n",
        "        scale_y: std dev for y-axis noise (overrides scale if provided)\n",
        "        seed   : random seed for reproducibility (default None)\n",
        "\n",
        "    Returns:\n",
        "        arr_noisy : copy of arr with noisy first two columns\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    arr_noisy = arr.copy()\n",
        "\n",
        "    # Use separate stddev if provided\n",
        "    sx = scale_x if scale_x is not None else scale\n",
        "    sy = scale_y if scale_y is not None else scale\n",
        "\n",
        "    noise_x = np.random.normal(loc=0.0, scale=sx, size=arr.shape[0])\n",
        "    noise_y = np.random.normal(loc=0.0, scale=sy, size=arr.shape[0])\n",
        "\n",
        "    arr_noisy[:, 0] += noise_x\n",
        "    arr_noisy[:, 1] += noise_y\n",
        "\n",
        "    # Clip to valid image coordinates\n",
        "    arr_noisy[:, 0] = np.clip(arr_noisy[:, 0], 0, W-1)  # x\n",
        "    arr_noisy[:, 1] = np.clip(arr_noisy[:, 1], 0, H-1)  # y\n",
        "\n",
        "    return arr_noisy\n"
      ],
      "metadata": {
        "id": "99Ixs5NETTQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def read_rgba_image_from_file(image_path: str, output_size: tuple[int, int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load an image as RGBA, resize it preserving aspect ratio,\n",
        "    and return a (1, H, W, 4) float32 array in [0, 1].\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        output_size (tuple[int, int]): Desired (width, height) of the output.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: (1, H, W, 4) float32 array in [0, 1].\n",
        "    \"\"\"\n",
        "    # --- 1Ô∏è‚É£ Load image as RGBA ---\n",
        "    img = Image.open(image_path).convert(\"RGBA\")\n",
        "\n",
        "    # --- 2Ô∏è‚É£ Resize preserving aspect ratio ---\n",
        "    out_w, out_h = output_size\n",
        "    img.thumbnail((out_w, out_h), Image.LANCZOS)\n",
        "\n",
        "    # --- 3Ô∏è‚É£ Center on transparent canvas of target size ---\n",
        "    canvas = Image.new(\"RGBA\", (out_w, out_h), (0, 0, 0, 0))\n",
        "    x = (out_w - img.width) // 2\n",
        "    y = (out_h - img.height) // 2\n",
        "    canvas.paste(img, (x, y))\n",
        "\n",
        "    # --- 4Ô∏è‚É£ Convert to numpy float32 in [0, 1] ---\n",
        "    rgba = np.array(canvas).astype(np.float32) / 255.0\n",
        "\n",
        "    # --- 5Ô∏è‚É£ Add batch dimension ---\n",
        "    return rgba[None, ...]  # (1, H, W, 4)\n"
      ],
      "metadata": {
        "id": "GfkYtJWFdyG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. üé® Create template RGB image and depth map"
      ],
      "metadata": {
        "id": "8aeDwjmrnd11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input image"
      ],
      "metadata": {
        "id": "iZ7oGngeYxft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the RGBA image file\n",
        "image_path = '/content/point3D_from_depth/assets/ChatGPT_Image.png'\n",
        "\n",
        "# Reads the RGBA image and resizes it to (H, W)\n",
        "# Assumes read_rgba_image_from_file returns a NumPy array (H, W, 4)\n",
        "rgb_cutout_resized = read_rgba_image_from_file(image_path=image_path, output_size=(H, W))\n",
        "\n",
        "# Remove extra dimensions if any (e.g., shape (1, H, W, 4) ‚Üí (H, W, 4))\n",
        "rgb_target = rgb_cutout_resized.squeeze()\n",
        "\n",
        "# Plot the image\n",
        "pl.figure()\n",
        "pl.axis('off')           # Remove axes for a clean display\n",
        "pl.imshow(rgb_target)    # Display the RGBA image\n",
        "pl.show()"
      ],
      "metadata": {
        "id": "5yGaeuIZbcO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üé• Camera Pose (Template/test Image)\n",
        "\n",
        "# Camera distance from object\n",
        "distance_template = 3.9  #@param {type:\"slider\", min:1.0, max:10.0, step:0.1}\n",
        "\n",
        "# Azimuth angle (horizontal rotation)\n",
        "azim_template = 11  #@param {type:\"slider\", min:-180.0, max:180.0, step:1.0}\n",
        "\n",
        "# Elevation angle (vertical tilt)\n",
        "elev_template = 26  #@param {type:\"slider\", min:-90.0, max:90.0, step:1.0}\n",
        "\n",
        "# Roll angle (rotation around camera axis)\n",
        "roll_template = 0  #@param {type:\"slider\", min:-180.0, max:180.0, step:1.0}\n",
        "\n",
        "print(f\"Camera pose:\\n  distance={distance_template}, azimuth={azim_template}, elevation={elev_template}, roll={roll_template}\")\n",
        "\n",
        "\n",
        "\n",
        "# Create a reference image\n",
        "rgb_template, depth_template, cams_template = \\\n",
        "      create_and_display_image(distance=distance_template,\n",
        "                               elev=elev_template,\n",
        "                               azim=azim_template,\n",
        "                               roll=roll_template,\n",
        "                               K = K_np,\n",
        "                               H = H,\n",
        "                               W = W)\n",
        "\n",
        "\n",
        "\n",
        "# We need these for later in the program\n",
        "device = cams_template.R.device\n",
        "dtype  = cams_template.R.dtype\n",
        "imgsz  = torch.tensor([[H, W]], device=device)"
      ],
      "metadata": {
        "id": "hr6oIpfEmvhp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **üßë‚Äçüíª User Input**: Detect matching features between the template image and the input image"
      ],
      "metadata": {
        "id": "Garjmi0pbQG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = unproject_tools.launch_point_matcher(rgb_template, rgb_target)"
      ],
      "metadata": {
        "id": "Tc9Uy2NEbPjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "ip = get_ipython()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Retrieve manually selected point correspondences\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "# Get points selected on the template image (e.g., reference view)\n",
        "template_points = ip.user_ns.get(\"selected_points_A\", [])\n",
        "\n",
        "# Get points selected on the target image (e.g., query view)\n",
        "target_points = ip.user_ns.get(\"selected_points_B\", [])\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Display the number of selected points\n",
        "# --------------------------------------------------------------\n",
        "print(\"\\n\")\n",
        "print(f\"Got {len(template_points)} points in A and {len(target_points)} points in B\\n\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Print table header\n",
        "# --------------------------------------------------------------\n",
        "print(f\"{'Idx':>3} |{'Point A (u,v)':>15}  |{'Point B (u,v)':>15}\")\n",
        "print(\"-\"*45)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Print each pair of corresponding points\n",
        "# --------------------------------------------------------------\n",
        "for i, (a, b) in enumerate(zip(template_points, target_points), 0):\n",
        "    # a = (u_A, v_A) in template image\n",
        "    # b = (u_B, v_B) in target image\n",
        "    print(f\"{i:3d} | ({a[0]:4d}, {a[1]:4d})    | ({b[0]:4d}, {b[1]:4d})\")\n",
        "\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "A6BynGfTeq-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate depth and 3-D coordinates `(x_world, y_world, z_world)`  for the selected points `(u,v)` + depth.\n",
        "\n",
        "\n",
        "Here, we use `cams.unproject(u,v,depth)` to recover the 3-D coordinates corresponding to the detected pixels. This steps gives us a set of 3-D object coordinates corresponding to the detected 2-D features.\n",
        "\n",
        "The estimated 3-D coordinates are then back-projected on the image for visualization."
      ],
      "metadata": {
        "id": "NbjOTmXES_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# üîπ Recover 3D coordinates in the world (object) coordinate system\n",
        "# --------------------------------------------------------------\n",
        "# Given the selected 2D points in the template image (`template_points`)\n",
        "# and the corresponding depth map (`depth_template`), recover their 3D\n",
        "# positions in the world (object) coordinate system using camera intrinsics/extrinsics.\n",
        "Xworld_unproject_t = recover_3D_from_depth_map(template_points, depth_template, cams_template)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Re-project 3D points back to 2D using the same camera\n",
        "# --------------------------------------------------------------\n",
        "# Clone and detach the 3D tensor to avoid in-place modifications or autograd tracking.\n",
        "x_world_new = Xworld_unproject_t.unsqueeze(0).detach().clone()  # Shape: (1, N, 3)\n",
        "\n",
        "# Use PyTorch3D's transform_points_screen to map 3D world points to 2D screen (pixel) coordinates.\n",
        "# The result `uvz` contains (u,v,z) for each point, where z is the depth in screen space.\n",
        "uvz = cams_template.transform_points_screen(x_world_new, image_size=imgsz)[0]  # Shape: (N, 3)\n",
        "\n",
        "# Extract only the 2D pixel coordinates (u, v)\n",
        "uv_back = uvz[:, :2]\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Plot reprojected points for visual validation\n",
        "# --------------------------------------------------------------\n",
        "# Overlay the reprojected 2D points (from 3D transform) on the template image\n",
        "myp3dtools.plot_re_projected_uv_on_image(\n",
        "    uv_back.cpu().numpy(),  # (N, 2)\n",
        "    rgb_template,           # background image\n",
        "    H, W,                   # image height and width\n",
        "    cams_template           # camera for plotting axes, etc.\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Console summary: compare 3D‚Äì2D correspondences\n",
        "# --------------------------------------------------------------\n",
        "print(\"\\n\")\n",
        "print(\"----------------------------------------------------------------------------------------\")\n",
        "print(\"      | World(x,y,z)            | Pixel (u,v)      | Pixel (u,v)\")\n",
        "print(\"Index | from p3d unproject()    | Ground-truth     | from transform_points_screen() \")\n",
        "print(\"----------------------------------------------------------------------------------------\")\n",
        "\n",
        "# Number of correspondences\n",
        "n = len(template_points)\n",
        "\n",
        "# Iterate over each correspondence\n",
        "for i, (pt_w, (u, v), (u_back, v_back)) in enumerate(\n",
        "    zip(Xworld_unproject_t, template_points, uv_back)\n",
        "):\n",
        "\n",
        "    # Highlight last few rows (optional styling)\n",
        "    # color = Fore.BLUE if i >= n - 5 else \"\"\n",
        "    color = Fore.BLACK if i >= n - 5 else \"\"\n",
        "\n",
        "    # Print comparison of 3D world coordinates and their 2D projections\n",
        "    print(f\"{color}{i:5d} | {pt_w[0]:+7.3f} {pt_w[1]:+7.3f} {pt_w[2]:+7.3f} | \"\n",
        "          f\"{u:7.2f} {v:7.2f}  | \"\n",
        "          f\"{u_back:7.2f} {v_back:7.2f} {Style.RESET_ALL}\")\n",
        "\n",
        "print(\"----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N-6ME-GXS_gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. üéØ Estimate pose pnp from 2-D-to-3-D correspondences"
      ],
      "metadata": {
        "id": "Bj0STRFCLRwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# üîπ Prepare 2D‚Äì3D correspondences for PnP pose estimation\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "# 2D points detected (or manually selected) in the target image\n",
        "# These are the image-space pixel coordinates of matched features.\n",
        "img_pts2d = target_points\n",
        "\n",
        "# Corresponding 3D points in the object (world) coordinate system\n",
        "# These come from unprojecting the template pixels via the depth map.\n",
        "obj_pts3d = Xworld_unproject_t.detach().cpu()  # (M, 3)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Estimate camera pose using Perspective-n-Point (PnP)\n",
        "# --------------------------------------------------------------\n",
        "# The goal is to find the camera rotation (R) and translation (T)\n",
        "# that best align the 3D world points (obj_pts3d) with their 2D\n",
        "# projections (img_pts2d) under the given camera intrinsics.\n",
        "#\n",
        "# - Uses OpenCV‚Äôs solvePnPRansac() internally for robust fitting\n",
        "# - Optionally refines the result with Levenberg‚ÄìMarquardt\n",
        "# - Returns R, T in PyTorch3D-compatible form (R_p3d, T_p3d)\n",
        "\n",
        "res = featmatchtools.estimate_pose_pnp(\n",
        "    mesh,                    # 3D mesh or reference object (optional context)\n",
        "    obj_pts3d,               # (M, 3) array/tensor of 3D world points\n",
        "    img_pts2d,               # (M, 2) array/tensor of corresponding 2D image points\n",
        "    fx, fy, cx, cy,          # Camera intrinsics (focal lengths and principal point)\n",
        "    W, H,                    # Image resolution (used for normalization)\n",
        "    # base_rgb=None,         # Optional background image for wireframe visualization\n",
        "    # wireframe_pts3d=None,  # Optional 3D vertices for drawing wireframe\n",
        "    # wireframe_edges=None,  # Optional connectivity list for wireframe plotting\n",
        "    ransac=True,             # Enable RANSAC for outlier rejection\n",
        "    refine=True,             # Refine final pose with nonlinear optimization\n",
        "    reproj_err=2,            # Maximum reprojection error (pixels) for RANSAC inlier threshold\n",
        "    iters=2000,              # Number of RANSAC iterations\n",
        "    pnp_flag=None,           # Optional override (e.g., cv2.SOLVEPNP_EPNP, AP3P, ITERATIVE)\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# üîπ Display numerical pose results\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "# RMS reprojection error ‚Äî a key quality metric for the pose\n",
        "# (lower is better; typically < 2 px is excellent)\n",
        "print(\"\\nRMS reprojection error (px):\", res[\"rms_px\"])\n",
        "\n",
        "# Rotation matrix (3√ó3) in PyTorch3D convention (world ‚Üí camera)\n",
        "print(\"\\nRecovered R (PyTorch3D):\\n\", res[\"R_p3d\"][0].cpu().numpy())\n",
        "\n",
        "# Translation vector (3√ó1) in PyTorch3D convention (world ‚Üí camera)\n",
        "print(\"\\nRecovered T (PyTorch3D):\\n\", res[\"T_p3d\"][0].cpu().numpy())\n"
      ],
      "metadata": {
        "id": "MWp1SqE3LRwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. üñºÔ∏è Render the object using the PnP estimate"
      ],
      "metadata": {
        "id": "txF-BlcALRwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# üîπ Import required libraries\n",
        "# =====================================================================\n",
        "import torch, numpy as np\n",
        "import cv2\n",
        "from pytorch3d.renderer import PerspectiveCameras\n",
        "from pytorch3d.utils import cameras_from_opencv_projection\n",
        "\n",
        "# =====================================================================\n",
        "# üîπ Convert OpenCV PnP results into PyTorch3D camera format\n",
        "# =====================================================================\n",
        "\n",
        "# OpenCV‚Äôs PnP output (from `estimate_pose_pnp`) provides transformation\n",
        "# matrices using OpenCV‚Äôs coordinate convention:\n",
        "#   - R_cv, t_cv describe the world‚Üícamera transform (same as PyTorch3D).\n",
        "#   - Units are consistent with the input 3D points.\n",
        "R_p3d = res[\"R_p3d\"]\n",
        "T_p3d = res[\"T_p3d\"]\n",
        "\n",
        "# Convert to NumPy for compatibility\n",
        "R_cv = R_p3d[0].detach().cpu().numpy()  # (3, 3)\n",
        "t_cv = T_p3d[0].detach().cpu().numpy()  # (3,)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ‚úÖ Create a PyTorch3D camera that exactly matches OpenCV‚Äôs projection\n",
        "# ---------------------------------------------------------------------\n",
        "# `cameras_from_opencv_projection` builds a PyTorch3D PerspectiveCameras\n",
        "# object directly from OpenCV-style extrinsics (R, t) and intrinsics (K).\n",
        "cams_pnp = cameras_from_opencv_projection(\n",
        "    R=torch.tensor(R_cv, dtype=torch.float32, device=device).unsqueeze(0),        # (1,3,3)\n",
        "    tvec=torch.tensor(t_cv, dtype=torch.float32, device=device).unsqueeze(0),     # (1,3)\n",
        "    camera_matrix=torch.tensor(K_np, dtype=torch.float32, device=device).unsqueeze(0),  # (1,3,3)\n",
        "    image_size=torch.tensor([[H, W]], dtype=torch.float32, device=device)         # (1,2)\n",
        ")\n",
        "\n",
        "# =====================================================================\n",
        "# üîπ Render the scene from the recovered PnP camera\n",
        "# =====================================================================\n",
        "# Generate both RGB and depth images of the mesh as seen from this camera.\n",
        "rgb_target_from_pnp, depth_target_from_pnp, cams_target_from_pnp = \\\n",
        "    unproject_tools.RenderWithPytorch3D.render_rgb_depth_from_view_from_RT(\n",
        "        mesh,\n",
        "        fx=fx, fy=fy, cx=cx, cy=cy,\n",
        "        width=W, height=H,\n",
        "        R=cams_pnp.R,\n",
        "        T=cams_pnp.T,\n",
        "    )\n",
        "\n",
        "# =====================================================================\n",
        "# üîπ Display and analyze the recovered camera\n",
        "# =====================================================================\n",
        "\n",
        "# Print rotation and translation matrices in readable format\n",
        "myp3dtools.print_camera_pose_matrices(\n",
        "    cams_pnp.R, cams_pnp.T, \"*** PyTorch3D Camera ***\"\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# üîπ Reproject known 3D world points through the estimated camera\n",
        "# ---------------------------------------------------------------------\n",
        "uvz_target_pnp = cams_pnp.transform_points_screen(x_world_new, image_size=imgsz)[0]  # (N, 3)\n",
        "uv_back_pnp = uvz_target_pnp[:, :2]  # Extract only pixel coordinates (u, v)\n",
        "\n",
        "# Plot reprojected points on the rendered RGB image to visualize alignment\n",
        "myp3dtools.plot_re_projected_uv_on_image(\n",
        "    uv_back_pnp.cpu().numpy(), rgb_target_from_pnp, H, W, cams_pnp\n",
        ")\n",
        "\n",
        "# Free unused CUDA memory (helpful for large renders)\n",
        "unproject_tools.Util.clear_cuda_cache()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ‚ö†Ô∏è Comparison placeholder (no ground-truth available)\n",
        "# ---------------------------------------------------------------------\n",
        "# This step simulates a comparison between the estimated PnP camera\n",
        "# and a \"reference\" camera (e.g., template view). In this notebook, the\n",
        "# actual object pose is unknown ‚Äî the template camera is used only as a\n",
        "# proxy to demonstrate how a ground-truth comparison would look.\n",
        "unproject_tools.print_extrinsics_comparison_color(\n",
        "    cams_template.R, cams_template.T, cams_pnp.R, cams_pnp.T\n",
        ")\n",
        "print(\"\\nRMS reprojection error (px):\", res[\"rms_px\"], \"\\n\")\n",
        "\n",
        "# =====================================================================\n",
        "# üîπ Visualize camera axes on the rendered and real target images\n",
        "# =====================================================================\n",
        "\n",
        "# Overlay world axes on the PyTorch3D render\n",
        "myp3dtools.overlay_axes_p3d(\n",
        "    rgb_target_from_pnp, cams_pnp, 256, 256,\n",
        "    world_origin=(0, 0, 0), axis_len=0.5,\n",
        "    draw_world_axes=True, draw_camera_axes=False,\n",
        "    cam_axis_len=0.5,\n",
        "    title=\"PyTorch3D camera\"\n",
        ")\n",
        "\n",
        "# Overlay the same camera axes directly on the actual target image\n",
        "myp3dtools.overlay_axes_p3d(\n",
        "    rgb_target, cams_pnp, 256, 256,\n",
        "    world_origin=(0, 0, 0), axis_len=0.5,\n",
        "    draw_world_axes=True, draw_camera_axes=False,\n",
        "    cam_axis_len=0.5,\n",
        "    title=\"PyTorch3D camera\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "0eMwF0AwoeXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}