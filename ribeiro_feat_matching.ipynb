{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMk879sATJU5bf2WhWZBZ+f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgXQ62E7IruA"
      },
      "source": [
        "# **Tutorial**: Feature matching\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![link text](https://learnopencv.com/wp-content/uploads/2025/03/image_matching_statue_mast3r_christ_precise_matching.jpg)\n",
        "**Figure 1**: Example of corresponding features detected from an object seen from largely different viewpoints (Figure from: https://learnopencv.com/mast3r-sfm-grounding-image-matching-3d/).\n",
        "\n",
        "Ideally, the detected points should correspond to the same physical features of the object, even when observed from significantly different viewpoints. Another key property of a robust feature detector and matcher is the ability to identify and match features across objects that are similar‚Äîbut not necessarily identical."
      ],
      "metadata": {
        "id": "MaZjJnKb4Om8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classical feature detectors/descriptors\n",
        "A number of classical detectors are implemented in the OpenCV library. See this page for examples: https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html."
      ],
      "metadata": {
        "id": "7gKveCyKuqXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep-learning based detectors/descriptors\n",
        "A good survey on feature detectors that use deep-learning can be found here: https://arxiv.org/pdf/2401.17592"
      ],
      "metadata": {
        "id": "laOp9sAfu6dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial code"
      ],
      "metadata": {
        "id": "CeRZZq5BvWZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mesh files\n",
        "\n",
        "\n",
        "We assume that the mesh file in `.ply` (or `.obj`) format are present in the directory `assets/`.\n",
        "\n",
        "The spaceship mesh file used in this tutorial was downloaded from: https://sketchfab.com/3d-models/spaceship-6164a883f57f4f13938c3c5999bc0e1f"
      ],
      "metadata": {
        "id": "YFY-xoxh4Ipm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkX7DiM6rmeM"
      },
      "source": [
        "## PyTorch3D in Colab\n",
        "\n",
        "Modules `torch` and `torchvision` are required. If `pytorch3d` is not installed, install it using the following cell. Here, I modified to install PyTorch3D from my own pre-built wheel. Using my own pytorch3d wheel allows for faster installation. Installing from source takes several minutes to complete.\n",
        "\n",
        "**‚ö†Ô∏è WARNING: If the PyTorch3D installation from the current wheel fails, create another one!!!**\n",
        "\n",
        "PyTorch3D takes a long time to install from source in Colab. Instead of installing from source everytime an Colab instance is started, this notebook uses a pre-built whell. The pre-built PyTorch3D wheel is downloaded from my Dropbox (shared link). Another copy of the wheel is also stored in my Google Drive, and is located at: `/content/drive/MyDrive/research/projects/slosh_project/slosh_project_team_files/Colab_wheels/`\n",
        "\n",
        "## Load the CAD model file\n",
        "\n",
        "We will load a CAD model (e.g., `ply` or `obj`) file and create a **Meshes** object. **Meshes** is a unique datastructure provided in PyTorch3D for working with **batches of meshes of different sizes**. It has several useful class methods which are used in the rendering pipeline.\n",
        "\n",
        "## Create a renderer\n",
        "\n",
        "A **renderer** in PyTorch3D is composed of a **rasterizer** and a **shader** which each have a number of subcomponents such as a **camera** (orthographic/perspective). Here, we initialize some of these components and use default values for the rest.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General settings (User input)\n",
        "\n",
        "\n",
        "‚ö†Ô∏è <b>Attention:</b> Replace the information with your GitHub email and username.\n",
        "\n",
        "\n",
        "‚ö†Ô∏è <b>Attention:</b> Press enter or run cells to accept default values.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "--AKB1LOLlpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings for GitHub Access\n",
        "\n",
        "# Set name and email for github cloning using #@param\n",
        "git_username = \"eraldoribeiro\" #@param {type:\"string\"}\n",
        "git_email = \"eribeiro@fit.edu\" #@param {type:\"string\"}\n",
        "\n",
        "repository_name = \"point3D_from_depth\" #@param {type:\"string\"}\n",
        "organization_name = \"ribeiro-computer-vision\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jae248wvLkJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to PyTorch3D (pre-built) wheel\n",
        "\n",
        "# Set name and email for github cloning using #@param\n",
        "dropbox_link = \"https://www.dropbox.com/scl/fi/fqvlnyponcbekjd01omhj/pytorch3d-0.7.8-cp312-cp312-linux_x86_64.whl?rlkey=563mfx35rog42z1c8y7qn31sk&dl=0\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "JPPOrO2KaAWB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to the Mast3r checkpoints file (Google Drive)\n",
        "\n",
        "checkpoints_gdrive_path = \"/content/drive/MyDrive/teaching/tutorials_files/mast3r_checkpoints\" #@param {type:\"string\"}\n",
        "\n",
        "checkpoints_file_name =  \"MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SualFq2Fqhg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to mesh file (.obj)\n",
        "\n",
        "# Path to mesh file\n",
        "obj_path = \"point3D_from_depth/assets/StarShip_small.obj\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u8YYtzbygADZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì∑ Camera Intrinsics\n",
        "# Focal lengths\n",
        "focal_length_x = 900  #@param {type:\"number\"}\n",
        "focal_length_y = 900  #@param {type:\"number\"}\n",
        "\n",
        "# Principal point\n",
        "principal_point_x = 128  #@param {type:\"number\"}\n",
        "principal_point_y = 128  #@param {type:\"number\"}\n",
        "\n",
        "# Image dimensions\n",
        "image_witdh = 256   #@param {type:\"number\"}\n",
        "image_height = 256  #@param {type:\"number\"}\n",
        "\n",
        "# --- Aliases for convenience in code ---\n",
        "fx, fy = focal_length_x, focal_length_y\n",
        "cx, cy = principal_point_x, principal_point_y\n",
        "\n",
        "print(\"\\nK =\")\n",
        "print(f\"[[{fx:8.2f} {0.0:8.2f} {cx:8.2f}]\")\n",
        "print(f\" [{0.0:8.2f} {fy:8.2f} {cy:8.2f}]\")\n",
        "print(f\" [{0.0:8.2f} {0.0:8.2f} {1.0:8.2f}]]\\n\")\n",
        "\n",
        "# --- Aliases for convenience in code ---\n",
        "W = image_witdh\n",
        "H = image_height"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ybeL8nQ7g1zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# ‚öôÔ∏è Setting up\n"
      ],
      "metadata": {
        "id": "jT1eWhCKCEzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to True if you want to mount gdrive\n",
        "mount_gdrive = True"
      ],
      "metadata": {
        "id": "BrwWhPGGGu6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWsl7eP3KdNj"
      },
      "outputs": [],
      "source": [
        "!pip --quiet install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÇ Clone Repository & üîë Mount Google Drive  & Install PyTorch3D/dependencies\n",
        "\n",
        "Clone the repository and mount **Google Drive** (requires user interaction).  \n",
        "This will also set up the environment and install the necessary libraries.\n"
      ],
      "metadata": {
        "id": "FSjobg6zKdNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set name and email for github cloning**\n",
        "\n",
        "<div style=\"border-left: 5px solid #FFA500; padding: 12px; background-color: #FFF4E5; font-size: 18px;\">\n",
        "  ‚ö†Ô∏è <b>Attention:</b> Replace the information with your GitHub email and username.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "KJ8iVQS3GItA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name git_username\n",
        "!git config --global user.email git_email"
      ],
      "metadata": {
        "id": "W1cT5w9BKdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîΩ Clone the Repository\n",
        "\n",
        "The next cell will **clone the repository** containing the notebooks and helper functions you‚Äôll need.  \n",
        "\n",
        "If the command fails (for example, due to missing secrets or permissions), you can open a **Terminal** in Colab and manually run the `git clone` command there.\n"
      ],
      "metadata": {
        "id": "46bBm7xFKdNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gh_clone(user, repo, token_key=\"GH_TOKEN\"):\n",
        "    from google.colab import userdata\n",
        "    token = userdata.get(token_key)\n",
        "    url = f\"https://{user}:{token}@github.com/{user}/{repo}.git\"\n",
        "    !git clone $url\n",
        "    %cd $repo\n",
        "    !git remote set-url origin $url\n",
        "    del token\n"
      ],
      "metadata": {
        "id": "spkzwj5DKdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell will **clone the repository** containing this notebooks and helper functions you‚Äôll need.\n",
        "\n",
        "If the `git clone` command fails (for example, due to missing secrets or permissions), you can open a **Terminal** in Colab and manually run the `git clone` command there.\n",
        "\n",
        "In Colab, we can only open a current notebook. But, we can edit python files (containing our library of functions) using git as we would normally when working on a computer. Any changes to files will only be saved to GitHub if we commit/push the changes prior to disconnecting the Colab instance. Colab sometimes disconnects without a warning so make sure the changes to files or notebooks are saved to github or google drive.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDLfvLkGV_M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gh_clone(organization_name, repository_name)\n",
        "\n",
        "# ‚úÖ Verify that the repository was cloned\n",
        "import os\n",
        "repo_name = \"/content/\" + repository_name\n",
        "if os.path.exists(repo_name):\n",
        "    print(f\"‚úÖ Repository '{repo_name}' successfully cloned!\")\n",
        "else:\n",
        "    print(f\"‚ùå Repository '{repo_name}' not found. Try cloning manually.\")"
      ],
      "metadata": {
        "id": "-50qkaXdV_NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîΩ Mount google drive"
      ],
      "metadata": {
        "id": "ZWoxDdyDKdNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaEFvu-dKdNk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "\n",
        "# auth.authenticate_user()\n",
        "\n",
        "local_path = os.getcwd()\n",
        "print(\"Current local path:\", local_path)\n",
        "\n",
        "# Mount google drive if using Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    local_path = \"/content/\"\n",
        "    from google.colab import drive\n",
        "    if mount_gdrive:\n",
        "        if mount_gdrive:\n",
        "            drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "\n",
        "os.chdir(local_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Install Pytorch3D"
      ],
      "metadata": {
        "id": "x6UF4bedKdNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ö° Install PyTorch3D from Wheel\n",
        "\n",
        "PyTorch3D installation can take longer than 8-10 minutes when installed from source.\n",
        "\n",
        "Here, **PyTorch3D is installed from a wheel** for a faster setup of about 2 minutes in Colab.\n",
        "\n",
        "- If the installer instead tries to **build from source**, it means the wheel is outdated or missing.  \n",
        "- In that case, you can **create your own wheel directly in Colab**, save it to **Google Drive** (or Dropbox), and reuse it later for faster installation.\n",
        "- To create your own PyTorch3D wheel in Colab, follow the instructions in the cell after these installation cells.\n",
        "\n"
      ],
      "metadata": {
        "id": "gmNGyDXgOqWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  ---------------------------- IMPORTS -----------------------------------------\n",
        "# Stdlib\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Literal, Dict, Any\n",
        "\n",
        "# Third-party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import imageio\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from tqdm.notebook import tqdm\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "# set path for libraries\n",
        "sys.path.append(repo_name)\n"
      ],
      "metadata": {
        "id": "gBN4D_j3KdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbDXl1qaKdNk"
      },
      "outputs": [],
      "source": [
        "# --- Config ---\n",
        "mount_gdrive = False\n",
        "\n",
        "# --- Imports ---\n",
        "import importlib, os, sys, shutil, subprocess, urllib.request, pathlib\n",
        "import installation_tools as install_tools\n",
        "importlib.reload(install_tools)\n",
        "\n",
        "# --- Short helpers (no notebook magics) ---\n",
        "def run(cmd, check=True):\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    try:\n",
        "        subprocess.run(cmd, check=check)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed ({e.returncode}): {' '.join(cmd)}\")\n",
        "        if check:\n",
        "            raise\n",
        "\n",
        "def pip_install(*pkgs, extra=None, check=True):\n",
        "    args = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
        "    if extra:\n",
        "        args += extra\n",
        "    args += list(pkgs)\n",
        "    run(args, check=check)\n",
        "\n",
        "def conda_available():\n",
        "    return shutil.which(\"conda\") is not None\n",
        "\n",
        "def conda_install(*pkgs):\n",
        "    if not conda_available():\n",
        "        print(\"conda not available; skipping conda installs.\")\n",
        "        return\n",
        "    # Use -c conda-forge channel and auto-yes\n",
        "    run([\"conda\", \"install\", \"-y\", \"-c\", \"conda-forge\", *pkgs], check=False)\n",
        "\n",
        "# --- Detect platform ---\n",
        "pm = install_tools.PlatformManager()\n",
        "platform, local_path = pm.platform, pm.local_path\n",
        "print(\"Detected:\", platform, local_path)\n",
        "\n",
        "# --- Optional: Mount GDrive if on Colab ---\n",
        "if mount_gdrive and platform == \"Colab\":\n",
        "    pm.mount_gdrive()\n",
        "\n",
        "# --- Lightning AI specific environment tweaks ---\n",
        "if platform == \"LightningAI\":\n",
        "    # conda piece (if conda exists in the image)\n",
        "    conda_install(\"libstdcxx-ng=13\")\n",
        "    # pip pins / extras\n",
        "    pip_install(\"numpy<2.0\", check=False)\n",
        "    pip_install(\"scikit-image\", \"gradio\", \"moviepy\", \"plotly\", check=False)\n",
        "    # If requirements.txt exists in CWD, install it\n",
        "    if os.path.exists(\"requirements.txt\"):\n",
        "        pip_install(\"-r\", \"requirements.txt\")\n",
        "\n",
        "# --- Install PyTorch3D (handles platform differences & fallbacks) ---\n",
        "installer = install_tools.PyTorch3DInstaller(\n",
        "    platform, local_path, dropbox_wheel_url=dropbox_link\n",
        ")\n",
        "installer.install()\n",
        "\n",
        "\n",
        "\n",
        "# --- Extra libraries (quiet-ish) ---\n",
        "# Original line had: trimesh pyrender opencv-python matplotlib pytorch-lightning\n",
        "pip_install(\"trimesh\", \"pyrender\", \"opencv-python\", \"matplotlib\", \"pytorch-lightning\", check=False)\n",
        "\n",
        "# --- Download plot_image_grid.py if missing ---\n",
        "filename = \"plot_image_grid.py\"\n",
        "url = \"https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\"\n",
        "if not os.path.exists(filename):\n",
        "    print(f\"Downloading {filename} ...\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(\"Saved to\", pathlib.Path(filename).resolve())\n",
        "    except Exception as e:\n",
        "        print(\"Download failed:\", e)\n",
        "\n",
        "# --- gdown ---\n",
        "pip_install(\"gdown\", extra=[\"--quiet\"], check=False)\n",
        "print(\"‚úÖ Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install and import colorama module (color printing)**"
      ],
      "metadata": {
        "id": "0U1UTXH6REb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "from colorama import Fore, Back, Style, init\n",
        "\n",
        "# ---------- pretty print helpers ----------\n",
        "RESET=\"\\033[0m\"; BOLD=\"\\033[1m\"\n",
        "C={\"ok\":\"\\033[1;32m\",\"info\":\"\\033[1;36m\",\"step\":\"\\033[1;35m\",\"warn\":\"\\033[1;33m\"}\n",
        "CYAN  = \"\\033[1;36m\"; GREEN = \"\\033[1;32m\"; YELLOW = \"\\033[1;33m\"\n",
        "\n",
        "\n",
        "def say(kind,msg): print(f\"{C[kind]}{msg}{RESET}\")\n",
        "torch.set_printoptions(precision=4, sci_mode=False)\n",
        "np.set_printoptions(precision=4, suppress=True)\n"
      ],
      "metadata": {
        "id": "aAPNSXLArV1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### üõ†Ô∏è (Optional) Build Your Own PyTorch3D Wheel\n",
        "\n",
        "If the pre-built wheel does not match your setup, you can **build PyTorch3D from source** and save the wheel to Google Drive.  \n",
        "This way, you only build once and reuse the `.whl` file in future Colab sessions.\n",
        "\n"
      ],
      "metadata": {
        "id": "VmfoO-IT1GWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Steps\n",
        "\n",
        "#### üîΩ 1. Clone PyTorch3D\n",
        "```python\n",
        "!git clone https://github.com/facebookresearch/pytorch3d.git\n",
        "%cd pytorch3d\n",
        "```\n",
        "#### üîΩ 2. Build the wheel (this may take several minutes)\n",
        "``` python\n",
        "!pip install build\n",
        "!python -m build --wheel\n",
        "```\n",
        "\n",
        "#### üîΩ 3. Find the wheel file\n",
        "``` python\n",
        "import glob, os\n",
        "wheels = glob.glob(\"dist/*.whl\")\n",
        "print(\"üì¶ Built wheels:\", wheels)\n",
        "```\n",
        "\n",
        "#### üîΩ 4. Copy the wheel to Google Drive (adjust path if needed)\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_path = \"/content/drive/MyDrive/pytorch3d_wheels/\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "for w in wheels:\n",
        "    !cp $w $save_path\n",
        "print(\"‚úÖ Wheel(s) saved to:\", save_path)\n",
        "```"
      ],
      "metadata": {
        "id": "U88mr5ZG3JN9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzgLJbG0BMak"
      },
      "source": [
        "#### PyTorch3D imports\n",
        "The following cell require PyTorch3D. Ensure it is executed after PyTorch3D is installed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ---------------------------- IMPORTS -----------------------------------------\n",
        "# PyTorch3D ‚Äî IO & data structures\n",
        "from pytorch3d.io import load_obj, load_ply, load_objs_as_meshes\n",
        "from pytorch3d.structures import Meshes\n",
        "\n",
        "# PyTorch3D ‚Äî transforms\n",
        "from pytorch3d.transforms import Rotate, Translate\n",
        "\n",
        "# PyTorch3D ‚Äî rendering\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    PerspectiveCameras,\n",
        "    look_at_view_transform,\n",
        "    look_at_rotation,\n",
        "    camera_position_from_spherical_angles,\n",
        "    RasterizationSettings,\n",
        "    MeshRenderer,\n",
        "    MeshRasterizer,\n",
        "    BlendParams,\n",
        "    SoftSilhouetteShader,\n",
        "    SoftPhongShader,\n",
        "    HardPhongShader,\n",
        "    PointLights,\n",
        "    DirectionalLights,\n",
        "    Materials,\n",
        "    TexturesUV,\n",
        "    TexturesVertex,\n",
        ")\n",
        "from pytorch3d.renderer.cameras import CamerasBase\n",
        "\n",
        "# PyTorch3D ‚Äî visualization helpers (optional)\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
        "\n",
        "# Project utils path (adjust as needed)\n",
        "sys.path.append(os.path.abspath(''))\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "qTLParFcKdNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions\n",
        "These function require PyTorch3D. As a result, they must be declared after PyTorch3D is installed."
      ],
      "metadata": {
        "id": "Uvg2B5WNKdNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import my own libraries and helper functions**\n"
      ],
      "metadata": {
        "id": "tlVxqQc739tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unproject_3d_from_depth_tools as unproject_tools\n",
        "importlib.reload(unproject_tools)\n",
        "\n",
        "import my_mast3r_setup as my_mast3r_tools\n",
        "importlib.reload(my_mast3r_tools)\n",
        "\n",
        "import tools_pytorch3d_coordsystems_demo as myp3dtools\n",
        "importlib.reload(unproject_tools)"
      ],
      "metadata": {
        "id": "zP_dmR7P36kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "SLIn2I6LLxp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def mast3r_match_features(template_image, cams_template, depth_template, target_image, model):\n",
        "\n",
        "\n",
        "    # We need these for later in the program\n",
        "    device = cams_template.R.device\n",
        "    dtype  = cams_template.R.dtype\n",
        "    # imgsz  = torch.tensor([[H, W]], device=device)\n",
        "\n",
        "    # Save rgb image to file for now to test feature detection directly.\n",
        "    # TODO: Change load_images function or create a new version that accepts image arrays instead of file names/directories\n",
        "    # img is a numpy array (H,W,3)\n",
        "    plt.imsave(path_to_images + \"template_image.png\", template_image)\n",
        "\n",
        "    # Save rgb image to file for now to test feature detection directly.\n",
        "    # Change load_images function or create a new version that accepts image arrays instead of file names/directories\n",
        "    # img is a numpy array (H,W,3)\n",
        "    plt.imsave(path_to_images + \"target_image.png\", target_image)\n",
        "\n",
        "    # Load image into list to create input for feature matcher\n",
        "    images = load_images([path_to_images + 'template_image.png', path_to_images + 'target_image.png'], size=256, square_ok=True)\n",
        "    print(images[0][\"img\"].shape)\n",
        "\n",
        "    print(\"***-------------------------------------------------------------- ***\")\n",
        "    print(\"**  3. Feature matching                                            **\")\n",
        "    print(\"***-------------------------------------------------------------- ***\")\n",
        "\n",
        "    # Inference\n",
        "    matches_template, matches_target, \\\n",
        "    view_template, pred_template, \\\n",
        "    view_target, pred_target = featmatchtools.FeatureMatcher.mast3r_inference(images = images,\n",
        "                                                              model = model,\n",
        "                                                              device = device)\n",
        "\n",
        "    # convert the view output from mast3r to standard rgb image\n",
        "    v_template = featmatchtools.FeatureMatcher.mast3r_view2rgbimage(view_template)\n",
        "    v_target   = featmatchtools.FeatureMatcher.mast3r_view2rgbimage(view_target)\n",
        "\n",
        "    # Create binary mask for this view (used for filtering spurious matches)\n",
        "    v_template_mask = unproject_tools.ImageProcessor.make_binary_mask(v_template, white_tol= 0, black_tol = 0)  # returns 0/1 mask\n",
        "    v_template_mask = featmatchtools.FeatureMatcher.smooth_and_fill_mask(v_template_mask)\n",
        "\n",
        "    # Create a binary mask for this view (used for filtering spurious matches)\n",
        "    v_target_mask = unproject_tools.ImageProcessor.make_binary_mask(v_target, white_tol= 0, black_tol = 0)  # returns 0/1 mask\n",
        "    v_target_mask = featmatchtools.FeatureMatcher.smooth_and_fill_mask(v_target_mask)\n",
        "\n",
        "    # Obtain a clean set of matches for each image (no matches in background)\n",
        "    matches_template_fg, matches_target_fg = featmatchtools.FeatureMatcher.filter_matches_by_mask(matches_template, matches_target, v_template_mask, v_target_mask)\n",
        "\n",
        "    # These (u,v) points are on the template image (coarse pose).\n",
        "    # They are the input to the 3-D unprojector from depth\n",
        "    points_uv_template = matches_template_fg\n",
        "\n",
        "    # Calculate depths for each (u,v) template point. We do not use this function\n",
        "    # to calculate 3-D coordinates or camera coordinates. However, we keep X_world\n",
        "    # for now, as we need it to pass to the clean-up function in the next step.\n",
        "    # We ignore camera coordinates as we use PyTorch3D's unproject_points() for\n",
        "    # the actual recovery of 3-D points from depth.\n",
        "    fx = cams_template.focal_length.cpu().numpy().squeeze()[0]\n",
        "    fy = cams_template.focal_length.cpu().numpy().squeeze()[1]\n",
        "    cx = cams_template.principal_point.cpu().numpy().squeeze()[0]\n",
        "    cy = cams_template.principal_point.cpu().numpy().squeeze()[1]\n",
        "    _, X_world, depths = \\\n",
        "      unproject_tools.Unprojector.recover_3D_points(matches_template_fg,\n",
        "                                                    depth_template.cpu(),\n",
        "                                                    fx, fy, cx, cy,\n",
        "                                                    cams_template.R,\n",
        "                                                    cams_template.T)\n",
        "\n",
        "    # Remove the matches for which 3-D reprojected coordinates have nan\n",
        "    # uv_clean are corresponding points to the actual test image (i.e., cutout image)\n",
        "    _, _, kept_idx, removed_idx = featmatchtools.FeatureMatcher.filter_nan_points(X_world, matches_template_fg)\n",
        "\n",
        "\n",
        "    template_points = matches_template_fg[kept_idx]\n",
        "    target_points = matches_target_fg[kept_idx]\n",
        "\n",
        "    return template_points, target_points, view_template, view_target\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Demo function to use for tests\n",
        "def create_and_display_image(distance=3,\n",
        "                             elev=0,\n",
        "                             azim=0,\n",
        "                             roll=0,\n",
        "                             K=np.eye(3),\n",
        "                             H=256,\n",
        "                             W=256):\n",
        "\n",
        "    # Get intrinsics\n",
        "    fx = K[0,0]\n",
        "    fy = K[1,1]\n",
        "    cx = K[0,2]\n",
        "    cy = K[1,2]\n",
        "\n",
        "    # Create rgb and depth images\n",
        "    rgb, depth, cams = unproject_tools.RenderWithPytorch3D.render_rgb_depth_from_view(\n",
        "        mesh,\n",
        "        fx=fx, fy=fy, cx=cx, cy=cy,\n",
        "        width=W, height=H,\n",
        "        distance=distance, elev=elev, azim=azim, roll_deg=roll,\n",
        "        roll_mode=\"camera\",   # try \"camera\" if you prefer or \"world\".\n",
        "    )\n",
        "\n",
        "    # Show\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1); plt.imshow(np.clip(rgb,0,1)); plt.axis('off'); plt.title('RGB')\n",
        "\n",
        "    # Depth visualization (treat -1 as invalid)\n",
        "    vis = unproject_tools.ImageProcessor.depth_to_rgb(depth, cmap=\"plasma\", bg_mode=\"white\")\n",
        "\n",
        "    plt.subplot(1,2,2); plt.imshow(vis); plt.axis('off'); plt.title('Depth')\n",
        "    plt.show()\n",
        "\n",
        "    #------------------------------- Show results ---------------------------------\n",
        "    myp3dtools.overlay_axes_p3d(rgb, cams, 256, 256,\n",
        "                    world_origin=(0,0,0), axis_len=0.3,\n",
        "                    draw_world_axes=True, draw_camera_axes=False,\n",
        "                    cam_axis_len=0.5,\n",
        "                    title=\"PyTorch3D camera\")\n",
        "\n",
        "    # Pretty print camera information\n",
        "    myp3dtools.print_camera_pose_matrices(cams.R, cams.T, \"*** PyTorch3D Camera ***\")\n",
        "\n",
        "    # clear gpu cache\n",
        "    unproject_tools.Util.clear_cuda_cache()\n",
        "\n",
        "    return rgb, depth, cams\n",
        "\n"
      ],
      "metadata": {
        "id": "PtSBhRlhsUOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CIbhy_uKdNl"
      },
      "source": [
        "### **Setup feature matcher (MAST3r)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install MAST3R\n",
        "my_mast3r_tools.setup_mast3r(checkpoint_gdrive_dir=checkpoints_gdrive_path,\n",
        "                             ckpt_name=checkpoints_file_name)\n",
        "\n",
        "# Create mast3r_images directory containing images for mast3r\n",
        "mast3r_images_dir = local_path + \"mast3r_images/\"\n",
        "if not os.path.exists(mast3r_images_dir):\n",
        "    os.makedirs(mast3r_images_dir)\n",
        "\n",
        "\n",
        "## Imports (from inside the mast3r directory)\n",
        "from mast3r.model import AsymmetricMASt3R\n",
        "from mast3r.fast_nn import fast_reciprocal_NNs\n",
        "\n",
        "import mast3r.utils.path_to_dust3r\n",
        "from dust3r.inference import inference\n",
        "from dust3r.utils.image import load_images\n",
        "\n",
        "\n",
        "# visualize a few matches\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms.functional\n",
        "from matplotlib import pyplot as pl\n",
        "\n",
        "import cv2\n",
        "import feature_matcher_tools as featmatchtools\n",
        "importlib.reload(featmatchtools)"
      ],
      "metadata": {
        "id": "jBqfv85eKdNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    path_to_images = \"/content/mast3r_images/\"\n",
        "else:\n",
        "    path_to_images = \"/teamspace/studios/this_studio/mast3r_images/\"\n",
        "\n",
        "print(f\"Path to image data is: {path_to_images}\")"
      ],
      "metadata": {
        "id": "szbKJMJzCetY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "schedule = 'cosine'\n",
        "lr = 0.01\n",
        "niter = 300\n",
        "\n",
        "model_name = \"naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric\"\n",
        "# you can put the path to a local checkpoint in model_name if needed\n",
        "model = AsymmetricMASt3R.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "6hiH6jnJGqZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read cad file into a PyTorch3D mesh"
      ],
      "metadata": {
        "id": "G_7s0Zc8KdNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "### Read cad file\n",
        "\n",
        "# Ensure we are in the home path\n",
        "os.chdir(local_path)\n",
        "\n",
        "# Load mesh\n",
        "mesh = load_objs_as_meshes([obj_path], device=device)"
      ],
      "metadata": {
        "id": "qYDBADlcKdNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the camera\n",
        "Ensure that this the only camera. PnP camera and rendering cameras must be the same."
      ],
      "metadata": {
        "id": "P8_nRIagKdNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Different type of matrices for different functions\n",
        "\n",
        "# Numpy K\n",
        "K_np = np.array([[fx, 0, cx],\n",
        "                 [0, fy, cy],\n",
        "                 [0,  0,  1]], dtype=np.float32)\n",
        "\n",
        "# Torch K\n",
        "K_torch = torch.tensor(K_np, dtype=torch.float32, device=device).unsqueeze(0)  # (1,3,3)"
      ],
      "metadata": {
        "id": "As2oQ2qvLnkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osYc7ucwmb24"
      },
      "source": [
        "## Steps prior to UI input of features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create target and test images and their depth maps"
      ],
      "metadata": {
        "id": "8aeDwjmrnd11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üé• Camera Pose (Target Image)\n",
        "\n",
        "# Camera distance from object\n",
        "distance_target = 2.7 # @param {\"type\":\"slider\",\"min\":1,\"max\":20,\"step\":0.1}\n",
        "\n",
        "# Azimuth angle (horizontal rotation)\n",
        "azim_target = 13  #@param {type:\"slider\", min:0.0, max:360.0, step:1.0}\n",
        "\n",
        "# Elevation angle (vertical tilt)\n",
        "elev_target = 0  #@param {type:\"slider\", min:-90.0, max:90.0, step:1.0}\n",
        "\n",
        "# Roll angle (rotation around camera axis)\n",
        "roll_target = 22  #@param {type:\"slider\", min:-180.0, max:180.0, step:1.0}\n",
        "\n",
        "print(f\"Camera pose:\\n  distance={distance_target}, azimuth={azim_target}, elevation={elev_target}, roll={roll_target}\")\n",
        "\n",
        "\n",
        "# Create a reference image\n",
        "rgb_target, depth_target, cams_target = \\\n",
        "      create_and_display_image(distance=distance_target,\n",
        "                               elev=elev_target,\n",
        "                               azim=azim_target,\n",
        "                               roll=roll_target,\n",
        "                               K = K_np,\n",
        "                               H = H,\n",
        "                               W = W)\n",
        "\n",
        "# We need these for later in the program\n",
        "device = cams_target.R.device\n",
        "dtype  = cams_target.R.dtype\n",
        "imgsz  = torch.tensor([[H, W]], device=device)"
      ],
      "metadata": {
        "id": "64cqTdAnPAe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üé• Camera Pose (Template/test Image)\n",
        "\n",
        "# Camera distance from object\n",
        "distance_template = 3  #@param {type:\"slider\", min:1.0, max:10.0, step:0.1}\n",
        "\n",
        "# Azimuth angle (horizontal rotation)\n",
        "azim_template = 19  #@param {type:\"slider\", min:0.0, max:360.0, step:1.0}\n",
        "\n",
        "# Elevation angle (vertical tilt)\n",
        "elev_template = 13  #@param {type:\"slider\", min:-90.0, max:90.0, step:1.0}\n",
        "\n",
        "# Roll angle (rotation around camera axis)\n",
        "roll_template = 21  #@param {type:\"slider\", min:-180.0, max:180.0, step:1.0}\n",
        "\n",
        "print(f\"Camera pose:\\n  distance={distance_template}, azimuth={azim_template}, elevation={elev_template}, roll={roll_template}\")\n",
        "\n",
        "\n",
        "\n",
        "# Create a reference image\n",
        "rgb_template, depth_template, cams_template = \\\n",
        "      create_and_display_image(distance=distance_template,\n",
        "                               elev=elev_template,\n",
        "                               azim=azim_template,\n",
        "                               roll=roll_template,\n",
        "                               K = K_np,\n",
        "                               H = H,\n",
        "                               W = W)"
      ],
      "metadata": {
        "id": "hr6oIpfEmvhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3al_ok8TrR6W"
      },
      "source": [
        "# Matching example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect and match features using MAST3R\n",
        "template_points, target_points, view_template, view_target = \\\n",
        "                mast3r_match_features(rgb_template,\n",
        "                                      cams_template,\n",
        "                                      depth_template,\n",
        "                                      rgb_target,\n",
        "                                      model)\n",
        "\n",
        "# Visualize matches\n",
        "featmatchtools.FeatureMatcher.visualize_mast3r_matches(template_points,\n",
        "                                                       target_points,\n",
        "                                                       view_template, view_target,\n",
        "                                                       n_viz_lines=50)\n"
      ],
      "metadata": {
        "id": "jnZ0YHYFypJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}