{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONmgWTlYm4OS4vlLDNRGg/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkX7DiM6rmeM"
      },
      "source": [
        "## PyTorch3D in Colab\n",
        "\n",
        "Modules `torch` and `torchvision` are required. If `pytorch3d` is not installed, install it using the following cell. Here, I modified to install PyTorch3D from my own pre-built wheel. Using my own pytorch3d wheel allows for faster installation. Installing from source takes several minutes to complete.\n",
        "\n",
        "**‚ö†Ô∏è WARNING: If the PyTorch3D installation from the current wheel fails, create another one!!!**\n",
        "\n",
        "PyTorch3D takes a long time to install from source in Colab. Instead of installing from source everytime an Colab instance is started, this notebook uses a pre-built whell. The pre-built PyTorch3D wheel is downloaded from my Dropbox (shared link). Another copy of the wheel is also stored in my Google Drive, and is located at: `/content/drive/MyDrive/research/projects/slosh_project/slosh_project_team_files/Colab_wheels/`\n",
        "\n",
        "## Load the CAD model file\n",
        "\n",
        "We will load a CAD model (e.g., `ply` or `obj`) file and create a **Meshes** object. **Meshes** is a unique datastructure provided in PyTorch3D for working with **batches of meshes of different sizes**. It has several useful class methods which are used in the rendering pipeline.\n",
        "\n",
        "## Create a renderer\n",
        "\n",
        "A **renderer** in PyTorch3D is composed of a **rasterizer** and a **shader** which each have a number of subcomponents such as a **camera** (orthographic/perspective). Here, we initialize some of these components and use default values for the rest.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General settings (User input)\n",
        "\n",
        "\n",
        "‚ö†Ô∏è <b>Attention:</b> Replace the information with your GitHub email and username.\n",
        "\n",
        "\n",
        "‚ö†Ô∏è <b>Attention:</b> Press enter or run cells to accept default values.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "--AKB1LOLlpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings for GitHub Access\n",
        "\n",
        "# Set name and email for github cloning using #@param\n",
        "git_username = \"eraldoribeiro\" #@param {type:\"string\"}\n",
        "git_email = \"eribeiro@fit.edu\" #@param {type:\"string\"}\n",
        "\n",
        "repository_name = \"point3D_from_depth\" #@param {type:\"string\"}\n",
        "organization_name = \"ribeiro-computer-vision\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jae248wvLkJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to PyTorch3D (pre-built) wheel\n",
        "\n",
        "# Set name and email for github cloning using #@param\n",
        "dropbox_link = \"https://www.dropbox.com/scl/fi/fqvlnyponcbekjd01omhj/pytorch3d-0.7.8-cp312-cp312-linux_x86_64.whl?rlkey=563mfx35rog42z1c8y7qn31sk&dl=0\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "JPPOrO2KaAWB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to the Mast3r checkpoints file (Google Drive)\n",
        "\n",
        "checkpoints_gdrive_path = \"/content/drive/MyDrive/teaching/tutorials_files/mast3r_checkpoints\" #@param {type:\"string\"}\n",
        "\n",
        "checkpoints_file_name =  \"MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SualFq2Fqhg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to mesh file (.obj)\n",
        "\n",
        "# Path to mesh file\n",
        "obj_path = \"point3D_from_depth/assets/StarShip_small.obj\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u8YYtzbygADZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì∑ Camera Intrinsics\n",
        "# Focal lengths\n",
        "focal_length_x = 900  #@param {type:\"number\"}\n",
        "focal_length_y = 900  #@param {type:\"number\"}\n",
        "\n",
        "# Principal point\n",
        "principal_point_x = 128  #@param {type:\"number\"}\n",
        "principal_point_y = 128  #@param {type:\"number\"}\n",
        "\n",
        "# Image dimensions\n",
        "image_witdh = 256   #@param {type:\"number\"}\n",
        "image_height = 256  #@param {type:\"number\"}\n",
        "\n",
        "# --- Aliases for convenience in code ---\n",
        "fx, fy = focal_length_x, focal_length_y\n",
        "cx, cy = principal_point_x, principal_point_y\n",
        "\n",
        "print(\"\\nK =\")\n",
        "print(f\"[[{fx:8.2f} {0.0:8.2f} {cx:8.2f}]\")\n",
        "print(f\" [{0.0:8.2f} {fy:8.2f} {cy:8.2f}]\")\n",
        "print(f\" [{0.0:8.2f} {0.0:8.2f} {1.0:8.2f}]]\\n\")\n",
        "\n",
        "# --- Aliases for convenience in code ---\n",
        "W = image_witdh\n",
        "H = image_height"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ybeL8nQ7g1zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# ‚öôÔ∏è Setting up\n"
      ],
      "metadata": {
        "id": "jT1eWhCKCEzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to True if you want to mount gdrive\n",
        "mount_gdrive = True"
      ],
      "metadata": {
        "id": "BrwWhPGGGu6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWsl7eP3KdNj"
      },
      "outputs": [],
      "source": [
        "!pip --quiet install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÇ Clone Repository & üîë Mount Google Drive  & Install PyTorch3D/dependencies\n",
        "\n",
        "Clone the repository and mount **Google Drive** (requires user interaction).  \n",
        "This will also set up the environment and install the necessary libraries.\n"
      ],
      "metadata": {
        "id": "FSjobg6zKdNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set name and email for github cloning**\n",
        "\n",
        "<div style=\"border-left: 5px solid #FFA500; padding: 12px; background-color: #FFF4E5; font-size: 18px;\">\n",
        "  ‚ö†Ô∏è <b>Attention:</b> Replace the information with your GitHub email and username.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "KJ8iVQS3GItA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name git_username\n",
        "!git config --global user.email git_email"
      ],
      "metadata": {
        "id": "W1cT5w9BKdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîΩ Clone the Repository\n",
        "\n",
        "The next cell will **clone the repository** containing the notebooks and helper functions you‚Äôll need.  \n",
        "\n",
        "If the command fails (for example, due to missing secrets or permissions), you can open a **Terminal** in Colab and manually run the `git clone` command there.\n"
      ],
      "metadata": {
        "id": "46bBm7xFKdNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gh_clone(user, repo, token_key=\"GH_TOKEN\"):\n",
        "    from google.colab import userdata\n",
        "    token = userdata.get(token_key)\n",
        "    url = f\"https://{user}:{token}@github.com/{user}/{repo}.git\"\n",
        "    !git clone $url\n",
        "    %cd $repo\n",
        "    !git remote set-url origin $url\n",
        "    del token\n"
      ],
      "metadata": {
        "id": "spkzwj5DKdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell will **clone the repository** containing this notebooks and helper functions you‚Äôll need.\n",
        "\n",
        "If the `git clone` command fails (for example, due to missing secrets or permissions), you can open a **Terminal** in Colab and manually run the `git clone` command there.\n",
        "\n",
        "In Colab, we can only open a current notebook. But, we can edit python files (containing our library of functions) using git as we would normally when working on a computer. Any changes to files will only be saved to GitHub if we commit/push the changes prior to disconnecting the Colab instance. Colab sometimes disconnects without a warning so make sure the changes to files or notebooks are saved to github or google drive.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDLfvLkGV_M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gh_clone(organization_name, repository_name)\n",
        "\n",
        "# ‚úÖ Verify that the repository was cloned\n",
        "import os\n",
        "repo_name = \"/content/\" + repository_name\n",
        "if os.path.exists(repo_name):\n",
        "    print(f\"‚úÖ Repository '{repo_name}' successfully cloned!\")\n",
        "else:\n",
        "    print(f\"‚ùå Repository '{repo_name}' not found. Try cloning manually.\")"
      ],
      "metadata": {
        "id": "-50qkaXdV_NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîΩ Mount google drive"
      ],
      "metadata": {
        "id": "ZWoxDdyDKdNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaEFvu-dKdNk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "\n",
        "# auth.authenticate_user()\n",
        "\n",
        "local_path = os.getcwd()\n",
        "print(\"Current local path:\", local_path)\n",
        "\n",
        "# Mount google drive if using Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    local_path = \"/content/\"\n",
        "    from google.colab import drive\n",
        "    if mount_gdrive:\n",
        "        if mount_gdrive:\n",
        "            drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "\n",
        "os.chdir(local_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Install Pytorch3D"
      ],
      "metadata": {
        "id": "x6UF4bedKdNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ö° Install PyTorch3D from Wheel\n",
        "\n",
        "PyTorch3D installation can take longer than 8-10 minutes when installed from source.\n",
        "\n",
        "Here, **PyTorch3D is installed from a wheel** for a faster setup of about 2 minutes in Colab.\n",
        "\n",
        "- If the installer instead tries to **build from source**, it means the wheel is outdated or missing.  \n",
        "- In that case, you can **create your own wheel directly in Colab**, save it to **Google Drive** (or Dropbox), and reuse it later for faster installation.\n",
        "- To create your own PyTorch3D wheel in Colab, follow the instructions in the cell after these installation cells.\n",
        "\n"
      ],
      "metadata": {
        "id": "gmNGyDXgOqWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  ---------------------------- IMPORTS -----------------------------------------\n",
        "# Stdlib\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Literal, Dict, Any\n",
        "\n",
        "# Third-party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import imageio\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from tqdm.notebook import tqdm\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "# set path for libraries\n",
        "sys.path.append(repo_name)\n"
      ],
      "metadata": {
        "id": "gBN4D_j3KdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbDXl1qaKdNk"
      },
      "outputs": [],
      "source": [
        "# --- Config ---\n",
        "mount_gdrive = False\n",
        "\n",
        "# --- Imports ---\n",
        "import importlib, os, sys, shutil, subprocess, urllib.request, pathlib\n",
        "import installation_tools as install_tools\n",
        "importlib.reload(install_tools)\n",
        "\n",
        "# --- Short helpers (no notebook magics) ---\n",
        "def run(cmd, check=True):\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    try:\n",
        "        subprocess.run(cmd, check=check)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed ({e.returncode}): {' '.join(cmd)}\")\n",
        "        if check:\n",
        "            raise\n",
        "\n",
        "def pip_install(*pkgs, extra=None, check=True):\n",
        "    args = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
        "    if extra:\n",
        "        args += extra\n",
        "    args += list(pkgs)\n",
        "    run(args, check=check)\n",
        "\n",
        "def conda_available():\n",
        "    return shutil.which(\"conda\") is not None\n",
        "\n",
        "def conda_install(*pkgs):\n",
        "    if not conda_available():\n",
        "        print(\"conda not available; skipping conda installs.\")\n",
        "        return\n",
        "    # Use -c conda-forge channel and auto-yes\n",
        "    run([\"conda\", \"install\", \"-y\", \"-c\", \"conda-forge\", *pkgs], check=False)\n",
        "\n",
        "# --- Detect platform ---\n",
        "pm = install_tools.PlatformManager()\n",
        "platform, local_path = pm.platform, pm.local_path\n",
        "print(\"Detected:\", platform, local_path)\n",
        "\n",
        "# --- Optional: Mount GDrive if on Colab ---\n",
        "if mount_gdrive and platform == \"Colab\":\n",
        "    pm.mount_gdrive()\n",
        "\n",
        "# --- Lightning AI specific environment tweaks ---\n",
        "if platform == \"LightningAI\":\n",
        "    # conda piece (if conda exists in the image)\n",
        "    conda_install(\"libstdcxx-ng=13\")\n",
        "    # pip pins / extras\n",
        "    pip_install(\"numpy<2.0\", check=False)\n",
        "    pip_install(\"scikit-image\", \"gradio\", \"moviepy\", \"plotly\", check=False)\n",
        "    # If requirements.txt exists in CWD, install it\n",
        "    if os.path.exists(\"requirements.txt\"):\n",
        "        pip_install(\"-r\", \"requirements.txt\")\n",
        "\n",
        "# --- Install PyTorch3D (handles platform differences & fallbacks) ---\n",
        "installer = install_tools.PyTorch3DInstaller(\n",
        "    platform, local_path, dropbox_wheel_url=dropbox_link\n",
        ")\n",
        "installer.install()\n",
        "\n",
        "\n",
        "\n",
        "# --- Extra libraries (quiet-ish) ---\n",
        "# Original line had: trimesh pyrender opencv-python matplotlib pytorch-lightning\n",
        "pip_install(\"trimesh\", \"pyrender\", \"opencv-python\", \"matplotlib\", \"pytorch-lightning\", check=False)\n",
        "\n",
        "# --- Download plot_image_grid.py if missing ---\n",
        "filename = \"plot_image_grid.py\"\n",
        "url = \"https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\"\n",
        "if not os.path.exists(filename):\n",
        "    print(f\"Downloading {filename} ...\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(\"Saved to\", pathlib.Path(filename).resolve())\n",
        "    except Exception as e:\n",
        "        print(\"Download failed:\", e)\n",
        "\n",
        "# --- gdown ---\n",
        "pip_install(\"gdown\", extra=[\"--quiet\"], check=False)\n",
        "print(\"‚úÖ Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install and import colorama module (color printing)**"
      ],
      "metadata": {
        "id": "0U1UTXH6REb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "from colorama import Fore, Back, Style, init\n",
        "\n",
        "# ---------- pretty print helpers ----------\n",
        "RESET=\"\\033[0m\"; BOLD=\"\\033[1m\"\n",
        "C={\"ok\":\"\\033[1;32m\",\"info\":\"\\033[1;36m\",\"step\":\"\\033[1;35m\",\"warn\":\"\\033[1;33m\"}\n",
        "CYAN  = \"\\033[1;36m\"; GREEN = \"\\033[1;32m\"; YELLOW = \"\\033[1;33m\"\n",
        "\n",
        "\n",
        "def say(kind,msg): print(f\"{C[kind]}{msg}{RESET}\")\n",
        "torch.set_printoptions(precision=4, sci_mode=False)\n",
        "np.set_printoptions(precision=4, suppress=True)\n"
      ],
      "metadata": {
        "id": "aAPNSXLArV1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### üõ†Ô∏è (Optional) Build Your Own PyTorch3D Wheel\n",
        "\n",
        "If the pre-built wheel does not match your setup, you can **build PyTorch3D from source** and save the wheel to Google Drive.  \n",
        "This way, you only build once and reuse the `.whl` file in future Colab sessions.\n",
        "\n"
      ],
      "metadata": {
        "id": "VmfoO-IT1GWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Steps\n",
        "\n",
        "#### üîΩ 1. Clone PyTorch3D\n",
        "```python\n",
        "!git clone https://github.com/facebookresearch/pytorch3d.git\n",
        "%cd pytorch3d\n",
        "```\n",
        "#### üîΩ 2. Build the wheel (this may take several minutes)\n",
        "``` python\n",
        "!pip install build\n",
        "!python -m build --wheel\n",
        "```\n",
        "\n",
        "#### üîΩ 3. Find the wheel file\n",
        "``` python\n",
        "import glob, os\n",
        "wheels = glob.glob(\"dist/*.whl\")\n",
        "print(\"üì¶ Built wheels:\", wheels)\n",
        "```\n",
        "\n",
        "#### üîΩ 4. Copy the wheel to Google Drive (adjust path if needed)\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_path = \"/content/drive/MyDrive/pytorch3d_wheels/\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "for w in wheels:\n",
        "    !cp $w $save_path\n",
        "print(\"‚úÖ Wheel(s) saved to:\", save_path)\n",
        "```"
      ],
      "metadata": {
        "id": "U88mr5ZG3JN9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzgLJbG0BMak"
      },
      "source": [
        "#### PyTorch3D imports\n",
        "The following cell require PyTorch3D. Ensure it is executed after PyTorch3D is installed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ---------------------------- IMPORTS -----------------------------------------\n",
        "# PyTorch3D ‚Äî IO & data structures\n",
        "from pytorch3d.io import load_obj, load_ply, load_objs_as_meshes\n",
        "from pytorch3d.structures import Meshes\n",
        "\n",
        "# PyTorch3D ‚Äî transforms\n",
        "from pytorch3d.transforms import Rotate, Translate\n",
        "\n",
        "# PyTorch3D ‚Äî rendering\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    PerspectiveCameras,\n",
        "    look_at_view_transform,\n",
        "    look_at_rotation,\n",
        "    camera_position_from_spherical_angles,\n",
        "    RasterizationSettings,\n",
        "    MeshRenderer,\n",
        "    MeshRasterizer,\n",
        "    BlendParams,\n",
        "    SoftSilhouetteShader,\n",
        "    SoftPhongShader,\n",
        "    HardPhongShader,\n",
        "    PointLights,\n",
        "    DirectionalLights,\n",
        "    Materials,\n",
        "    TexturesUV,\n",
        "    TexturesVertex,\n",
        ")\n",
        "from pytorch3d.renderer.cameras import CamerasBase\n",
        "\n",
        "# PyTorch3D ‚Äî visualization helpers (optional)\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
        "\n",
        "# Project utils path (adjust as needed)\n",
        "sys.path.append(os.path.abspath(''))\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "qTLParFcKdNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start of the original tutorial (PyTorch3D)\n",
        "Original notebook: https://pytorch3d.org/tutorials/bundle_adjustment"
      ],
      "metadata": {
        "id": "6XFeagmej6ym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD6DUkgzmFoR"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj6j6__ZmFoW"
      },
      "source": [
        "# Absolute camera orientation given set of relative camera pairs\n",
        "\n",
        "This tutorial showcases the `cameras`, `transforms` and `so3` API.\n",
        "\n",
        "The problem we deal with is defined as follows:\n",
        "\n",
        "Given an optical system of $N$ cameras with extrinsics $\\{g_1, ..., g_N | g_i \\in SE(3)\\}$, and a set of relative camera positions $\\{g_{ij} | g_{ij}\\in SE(3)\\}$ that map between coordinate frames of randomly selected pairs of cameras $(i, j)$, we search for the absolute extrinsic parameters $\\{g_1, ..., g_N\\}$ that are consistent with the relative camera motions.\n",
        "\n",
        "More formally:\n",
        "$$\n",
        "g_1, ..., g_N =\n",
        "{\\arg \\min}_{g_1, ..., g_N} \\sum_{g_{ij}} d(g_{ij}, g_i^{-1} g_j),\n",
        "$$,\n",
        "where $d(g_i, g_j)$ is a suitable metric that compares the extrinsics of cameras $g_i$ and $g_j$.\n",
        "\n",
        "Visually, the problem can be described as follows. The picture below depicts the situation at the beginning of our optimization. The ground truth cameras are plotted in purple while the randomly initialized estimated cameras are plotted in orange:\n",
        "![Initialization](https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/data/bundle_adjustment_initialization.png?raw=1)\n",
        "\n",
        "Our optimization seeks to align the estimated (orange) cameras with the ground truth (purple) cameras, by minimizing the discrepancies between pairs of relative cameras. Thus, the solution to the problem should look as follows:\n",
        "![Solution](https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/data/bundle_adjustment_final.png?raw=1)\n",
        "\n",
        "In practice, the camera extrinsics $g_{ij}$ and $g_i$ are represented using objects from the `SfMPerspectiveCameras` class initialized with the corresponding rotation and translation matrices `R_absolute` and `T_absolute` that define the extrinsic parameters $g = (R, T); R \\in SO(3); T \\in \\mathbb{R}^3$. In order to ensure that `R_absolute` is a valid rotation matrix, we represent it using an exponential map (implemented with `so3_exp_map`) of the axis-angle representation of the rotation `log_R_absolute`.\n",
        "\n",
        "Note that the solution to this problem could only be recovered up to an unknown global rigid transformation $g_{glob} \\in SE(3)$. Thus, for simplicity, we assume knowledge of the absolute extrinsics of the first camera $g_0$. We set $g_0$ as a trivial camera $g_0 = (I, \\vec{0})$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAQY4EnHmFoX"
      },
      "source": [
        "## 0. Install and Import Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAHR1LMJmP-h"
      },
      "source": [
        "Ensure `torch` and `torchvision` are installed. If `pytorch3d` is not installed, install it using the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo7a3gdImMZx"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import sys\n",
        "# import torch\n",
        "# import subprocess\n",
        "# need_pytorch3d=False\n",
        "# try:\n",
        "#     import pytorch3d\n",
        "# except ModuleNotFoundError:\n",
        "#     need_pytorch3d=True\n",
        "# if need_pytorch3d:\n",
        "#     pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "#     version_str=\"\".join([\n",
        "#         f\"py3{sys.version_info.minor}_cu\",\n",
        "#         torch.version.cuda.replace(\".\",\"\"),\n",
        "#         f\"_pyt{pyt_version_str}\"\n",
        "#     ])\n",
        "#     !pip install iopath\n",
        "#     if sys.platform.startswith(\"linux\"):\n",
        "#         print(\"Trying to install wheel for PyTorch3D\")\n",
        "#         !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "#         pip_list = !pip freeze\n",
        "#         need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for  i in pip_list)\n",
        "#     if need_pytorch3d:\n",
        "#         print(f\"failed to find/install wheel for {version_str}\")\n",
        "# if need_pytorch3d:\n",
        "#     print(\"Installing PyTorch3D from source\")\n",
        "#     !pip install ninja\n",
        "#     !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgLa7XQimFoY"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "from pytorch3d.transforms.so3 import (\n",
        "    so3_exp_map,\n",
        "    so3_relative_angle,\n",
        ")\n",
        "from pytorch3d.renderer.cameras import (\n",
        "    SfMPerspectiveCameras,\n",
        ")\n",
        "\n",
        "# add path for demo utils\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(''))\n",
        "\n",
        "# set for reproducibility\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"WARNING: CPU only, this will be slow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4emnRuzmpRB"
      },
      "source": [
        "If using **Google Colab**, fetch the utils file for plotting the camera scene, and the ground truth camera positions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOvMPYJdmd15"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/camera_visualization.py\n",
        "from camera_visualization import plot_camera_scene\n",
        "\n",
        "!mkdir data\n",
        "!wget -P data https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/data/camera_graph.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9WD5vaimw3K"
      },
      "source": [
        "OR if running **locally** uncomment and run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucGlQj5EmmJ5"
      },
      "outputs": [],
      "source": [
        "# from utils import plot_camera_scene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WeEi7IgmFoc"
      },
      "source": [
        "## 1. Set up Cameras and load ground truth positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_Wm0zikmFod"
      },
      "outputs": [],
      "source": [
        "# load the SE3 graph of relative/absolute camera positions\n",
        "camera_graph_file = './data/camera_graph.pth'\n",
        "(R_absolute_gt, T_absolute_gt), \\\n",
        "    (R_relative, T_relative), \\\n",
        "    relative_edges = \\\n",
        "        torch.load(camera_graph_file)\n",
        "\n",
        "# create the relative cameras\n",
        "cameras_relative = SfMPerspectiveCameras(\n",
        "    R = R_relative.to(device),\n",
        "    T = T_relative.to(device),\n",
        "    device = device,\n",
        ")\n",
        "\n",
        "# create the absolute ground truth cameras\n",
        "cameras_absolute_gt = SfMPerspectiveCameras(\n",
        "    R = R_absolute_gt.to(device),\n",
        "    T = T_absolute_gt.to(device),\n",
        "    device = device,\n",
        ")\n",
        "\n",
        "# the number of absolute camera positions\n",
        "N = R_absolute_gt.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f-RNlGemFog"
      },
      "source": [
        "## 2. Define optimization functions\n",
        "\n",
        "### Relative cameras and camera distance\n",
        "We now define two functions crucial for the optimization.\n",
        "\n",
        "**`calc_camera_distance`** compares a pair of cameras. This function is important as it defines the loss that we are minimizing. The method utilizes the `so3_relative_angle` function from the SO3 API.\n",
        "\n",
        "**`get_relative_camera`** computes the parameters of a relative camera that maps between a pair of absolute cameras. Here we utilize the `compose` and `inverse` class methods from the PyTorch3D Transforms API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzzk88RHmFoh"
      },
      "outputs": [],
      "source": [
        "def calc_camera_distance(cam_1, cam_2):\n",
        "    \"\"\"\n",
        "    Calculates the divergence of a batch of pairs of cameras cam_1, cam_2.\n",
        "    The distance is composed of the cosine of the relative angle between\n",
        "    the rotation components of the camera extrinsics and the l2 distance\n",
        "    between the translation vectors.\n",
        "    \"\"\"\n",
        "    # rotation distance\n",
        "    R_distance = (1.-so3_relative_angle(cam_1.R, cam_2.R, cos_angle=True)).mean()\n",
        "    # translation distance\n",
        "    T_distance = ((cam_1.T - cam_2.T)**2).sum(1).mean()\n",
        "    # the final distance is the sum\n",
        "    return R_distance + T_distance\n",
        "\n",
        "def get_relative_camera(cams, edges):\n",
        "    \"\"\"\n",
        "    For each pair of indices (i,j) in \"edges\" generate a camera\n",
        "    that maps from the coordinates of the camera cams[i] to\n",
        "    the coordinates of the camera cams[j]\n",
        "    \"\"\"\n",
        "\n",
        "    # first generate the world-to-view Transform3d objects of each\n",
        "    # camera pair (i, j) according to the edges argument\n",
        "    trans_i, trans_j = [\n",
        "        SfMPerspectiveCameras(\n",
        "            R = cams.R[edges[:, i]],\n",
        "            T = cams.T[edges[:, i]],\n",
        "            device = device,\n",
        "        ).get_world_to_view_transform()\n",
        "         for i in (0, 1)\n",
        "    ]\n",
        "\n",
        "    # compose the relative transformation as g_i^{-1} g_j\n",
        "    trans_rel = trans_i.inverse().compose(trans_j)\n",
        "\n",
        "    # generate a camera from the relative transform\n",
        "    matrix_rel = trans_rel.get_matrix()\n",
        "    cams_relative = SfMPerspectiveCameras(\n",
        "                        R = matrix_rel[:, :3, :3],\n",
        "                        T = matrix_rel[:, 3, :3],\n",
        "                        device = device,\n",
        "                    )\n",
        "    return cams_relative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys9J7MbMmFol"
      },
      "source": [
        "## 3. Optimization\n",
        "Finally, we start the optimization of the absolute cameras.\n",
        "\n",
        "We use SGD with momentum and optimize over `log_R_absolute` and `T_absolute`.\n",
        "\n",
        "As mentioned earlier, `log_R_absolute` is the axis angle representation of the rotation part of our absolute cameras. We can obtain the 3x3 rotation matrix `R_absolute` that corresponds to `log_R_absolute` with:\n",
        "\n",
        "`R_absolute = so3_exp_map(log_R_absolute)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOK_DUzVmFom"
      },
      "outputs": [],
      "source": [
        "# initialize the absolute log-rotations/translations with random entries\n",
        "log_R_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n",
        "T_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n",
        "\n",
        "# furthermore, we know that the first camera is a trivial one\n",
        "#    (see the description above)\n",
        "log_R_absolute_init[0, :] = 0.\n",
        "T_absolute_init[0, :] = 0.\n",
        "\n",
        "# instantiate a copy of the initialization of log_R / T\n",
        "log_R_absolute = log_R_absolute_init.clone().detach()\n",
        "log_R_absolute.requires_grad = True\n",
        "T_absolute = T_absolute_init.clone().detach()\n",
        "T_absolute.requires_grad = True\n",
        "\n",
        "# the mask the specifies which cameras are going to be optimized\n",
        "#     (since we know the first camera is already correct,\n",
        "#      we only optimize over the 2nd-to-last cameras)\n",
        "camera_mask = torch.ones(N, 1, dtype=torch.float32, device=device)\n",
        "camera_mask[0] = 0.\n",
        "\n",
        "# init the optimizer\n",
        "optimizer = torch.optim.SGD([log_R_absolute, T_absolute], lr=.1, momentum=0.9)\n",
        "\n",
        "# run the optimization\n",
        "n_iter = 5000  # fix the number of iterations\n",
        "for it in range(n_iter):\n",
        "    # re-init the optimizer gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # compute the absolute camera rotations as\n",
        "    # an exponential map of the logarithms (=axis-angles)\n",
        "    # of the absolute rotations\n",
        "    R_absolute = so3_exp_map(log_R_absolute * camera_mask)\n",
        "\n",
        "    # get the current absolute cameras\n",
        "    cameras_absolute = SfMPerspectiveCameras(\n",
        "        R = R_absolute,\n",
        "        T = T_absolute * camera_mask,\n",
        "        device = device,\n",
        "    )\n",
        "\n",
        "    # compute the relative cameras as a composition of the absolute cameras\n",
        "    cameras_relative_composed = \\\n",
        "        get_relative_camera(cameras_absolute, relative_edges)\n",
        "\n",
        "    # compare the composed cameras with the ground truth relative cameras\n",
        "    # camera_distance corresponds to $d$ from the description\n",
        "    camera_distance = \\\n",
        "        calc_camera_distance(cameras_relative_composed, cameras_relative)\n",
        "\n",
        "    # our loss function is the camera_distance\n",
        "    camera_distance.backward()\n",
        "\n",
        "    # apply the gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # plot and print status message\n",
        "    if it % 200==0 or it==n_iter-1:\n",
        "        status = 'iteration=%3d; camera_distance=%1.3e' % (it, camera_distance)\n",
        "        plot_camera_scene(cameras_absolute, cameras_absolute_gt, status)\n",
        "\n",
        "print('Optimization finished.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncLMvxWnhmO"
      },
      "source": [
        "## 4. Conclusion\n",
        "\n",
        "In this tutorial we learnt how to initialize a batch of SfM Cameras, set up loss functions for bundle adjustment, and run an optimization loop."
      ]
    }
  ]
}